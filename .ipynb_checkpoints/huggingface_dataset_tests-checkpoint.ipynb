{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97783",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Ensure that during pre training, both sentences fit into the model at the same time -> DONE but not teted\n",
    "* think about visualizations for text, preprocessing text, etc.\n",
    "* implement interfaces of the task sheet\n",
    "* cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64 # maximum sequence length\n",
    "VOCAB_SIZE = 30522  # = len(tokenizer.vocab)\n",
    "N_SEGMENTS = 3 # number of segmentation labels\n",
    "EMBED_SIZE = 768 # size of embedding vector\n",
    "DROPOUT = 0.1 # dropout chance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464fe5e",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc737c1",
   "metadata": {},
   "source": [
    "### bookcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b594b1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (C:/Users/Johannes/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6261cb8226b249289d6020ed7bed5eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 74004228\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Download + load data from cache or online AUTOMATICALLY\n",
    "# https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bookcorpus\") # alternative, less size datasets.load_dataset(\"bookcorpus\", split=\"train[:10%]\")\n",
    "# split=\"train[10:20]\")\n",
    "# saved here on windows C:\\Users\\morit\\.cache\\huggingface\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f56493",
   "metadata": {},
   "source": [
    "#### Saving huggingface Dataset on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a6ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual save to disk\n",
    "\n",
    "#folder_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\pretraining\"\n",
    "#full_path = folder_path+r\"\\bookcorpus\"\n",
    "\n",
    "#dataset.save_to_disk(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cfd29",
   "metadata": {},
   "source": [
    "#### Loading hf dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a77b037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manual load from disk\n",
    "\n",
    "#dataset = datasets.load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4604f83",
   "metadata": {},
   "source": [
    "#### slicing hf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14821a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her parents rattled along to each other as they made their way through the tree-lined suburbs where megan had grown up .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][66][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c4a55",
   "metadata": {},
   "source": [
    "#### Standard dataloader - not sufficient we need tokenized output -> implement own dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15cb2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['usually , he would be tearing around the living room , playing with his toys .',\n",
       "  'but just one look at a minion sent him practically catatonic .']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset[\"train\"], batch_size=2)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebacda",
   "metadata": {},
   "source": [
    "#### Tokenizer - use pretrained, at least for prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose an appropriate tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model_max_length = SEQ_LEN # might not be correct in case of pretraining where we add CLS at the end, check that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827bc7f",
   "metadata": {},
   "source": [
    "#### Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cc5fe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncation_side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5adf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # we might need to fixate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2db1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09c4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a719881",
   "metadata": {},
   "source": [
    "#### Tokenizer example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6afd8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am moritz, who are you ?\"#[\"hi i am moritz\", \"no you are not moritz, you are kevin\"]\n",
    "encoded_input = tokenizer(text)#,padding=True, truncation=True)\n",
    "# , return_tensors='pt') use this for pt tensors\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "432fff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51f874a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi i am moritz, who are you? [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b4d7e",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20979eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = None\n",
    "n_rows is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8564c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "class Bookcorpus(Dataset): # TODO rewrite \n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "        \"\"\"\n",
    "        n_rows == None means take the whole dataset\n",
    "        \"\"\"\n",
    "     \n",
    "        if not split in [\"train\"]:\n",
    "            raise ValueError(\"For Bookcorpus there is only a train split\")\n",
    "            \n",
    "        self.n_rows = n_rows # is only inititialized if __len__() is called\n",
    "        self.tokenizer = tokenizer \n",
    "        self.seq_len = seq_len\n",
    "        self.split = split\n",
    "        self.dataset = None # only loaded id needed\n",
    "    \n",
    "    # apply lazy loading\n",
    "    def load_memory(self):\n",
    "        if self.n_rows is not None:\n",
    "            self.dataset = load_dataset(\"bookcorpus\", split=self.split+\"[0:\"+str(self.n_rows)+\"]\") # [split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"bookcorpus\") # [split]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset is None:\n",
    "            self.load_memory() # only loaded if required\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        if self.dataset is None:\n",
    "            self.load_data() # only loaded if required\n",
    "        \n",
    "        # Create a random pair of sentences, if subseq is true if they are subsequent\n",
    "        s1, s2, subseq = self.get_sentence_pair(item)\n",
    "        \n",
    "        # Replace 15% of the words in each line with masks/random words/the word itself\n",
    "        s1_random, s1_label = self.random_masking(s1)\n",
    "        s2_random, s2_label = self.random_masking(s2)\n",
    "        \n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences # copied \n",
    "         # Adding PAD token for labels\n",
    "        cls = [self.tokenizer.vocab['[CLS]']]\n",
    "        sep = [self.tokenizer.vocab['[SEP]']] \n",
    "        pad = [self.tokenizer.vocab['[PAD]']]\n",
    "        \n",
    "        # append separating tokens to sequence       \n",
    "        s1 = cls + s1_random + sep       \n",
    "        s2 = s2_random + sep\n",
    "        s1_label = pad + s1_label + pad\n",
    "        s2_label = s2_label + pad\n",
    "               \n",
    "        # add segement label, adding padding\n",
    "        segment = ([1 for i in range(len(s1))]+[2 for i in range(len(s2))])[:self.seq_len]\n",
    "        # generate 1 input for model\n",
    "        model_input = (s1+s2)[:self.seq_len]\n",
    "        model_label = (s1_label + s2_label)[:self.seq_len]\n",
    "        # add padding where input is shorter than sequence\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(model_input))]\n",
    "        model_input.extend(padding)\n",
    "        model_label.extend(padding)\n",
    "        segment.extend(padding)\n",
    "        \n",
    "\n",
    "\n",
    "        output = {\n",
    "            \"input\": torch.tensor(model_input),\n",
    "            \"label\": torch.tensor(model_label),\n",
    "            \"segment\": torch.tensor(segment),\n",
    "            \"subseq\": torch.tensor(subseq)\n",
    "        }\n",
    "\n",
    "        return {key: value.clone().detach() for key, value in output.items()}        \n",
    "        #return  {\"s1\":s1, \"s2\":s2, \"is_next_label\":is_next_label}\n",
    "        #return {\"t1_random\":t1_random, \"t1_label\":t1_label, \"t2_random\":t2_random, \"t2_label\":t2_label}\n",
    "    \n",
    "    def get_sentence_pair(self, index): \n",
    "        '''gets sentence pair as dicitinary s1, s2, isNext'''\n",
    "        isNext = random.random() > 0.5 # if number > 0.5 isNext is positive\n",
    "        \n",
    "        t1 = self.dataset[index][\"text\"]\n",
    "        if isNext: # select two subsequent lines\n",
    "            t2 = self.dataset[index+1][\"text\"]\n",
    "            return t1, t2, 1 # line1, line2, subsequent\n",
    "        else: # select two non-Subsequent lines (index+1 is excluded from random selection)\n",
    "            t2 = self.get_random_line(index+1)[\"text\"]\n",
    "            return t1, t2, 0 # line1, line2, subsequent\n",
    "        \n",
    "    def get_random_line(self, excludedIndex): \n",
    "        '''return random single sentence excluding'''\n",
    "        randIndex = random.randint(1, self.__len__())\n",
    "            \n",
    "        # ensure that randIndex is not next sentence\n",
    "        while randIndex == excludedIndex:\n",
    "            randIndex = random.randint(1, self.__len__())\n",
    "        \n",
    "        return self.dataset[randIndex]\n",
    "    \n",
    "    def random_masking(self, sentence):\n",
    "        words = sentence.split()\n",
    "        masked_out = []\n",
    "        masked_labels = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            rnd_number1 = random.random() # continuous number from [0,1]\n",
    "            rnd_number2 = random.random() # continuous number from [0,1]\n",
    "\n",
    "            # turn word into token, remove [CLS], [SEP]\n",
    "            token = self.tokenizer(word)['input_ids'] \n",
    "            token = token[1:-1]\n",
    "\n",
    "            # replace a word with a probability of 15%\n",
    "            if rnd_number1 < 0.15:\n",
    "\n",
    "                # with 80% chance replace word by mask\n",
    "                if rnd_number2 < 0.8:\n",
    "                    for j in range(len(token)):\n",
    "                        masked_out.append(self.tokenizer.vocab['[MASK]'])\n",
    "                # with 10% chance replace word by random word\n",
    "                elif rnd_number2 < 0.9:\n",
    "                    for k in range(len(token)):\n",
    "                        masked_out.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "                # with 10% chance word remains\n",
    "                else:\n",
    "                    masked_out.append(token)\n",
    "\n",
    "                # set corresponding label\n",
    "                masked_labels.append(token)\n",
    "            # 85% don't change anything\n",
    "            else:\n",
    "                masked_out.append(token)\n",
    "                # create corrsponding 0-label\n",
    "                for l in range(len(token)):\n",
    "                    masked_labels.append(0)\n",
    "                \n",
    "        # flatten output\n",
    "        masked_out = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in masked_out]))\n",
    "        print(masked_out)\n",
    "        masked_labels = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in masked_labels]))\n",
    "        print(masked_labels)\n",
    "\n",
    "        # check for correct length\n",
    "        assert len(masked_out) == len(masked_labels)\n",
    "        #assert len(output) == self.seq_len, \"sequence length not fixed! \"+str(len(output)) # from moritz\n",
    "        return masked_out, masked_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e21d7",
   "metadata": {},
   "source": [
    "#### Testing the Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40334c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (C:/Users/Johannes/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Bookcorpus(tokenizer, n_rows = 100)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b123188",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(test,batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed0ac1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(1,1000):\\n    batch = next(iter(dl))\\n    for j in range(1,2): # batchsize\\n        length_ = len(batch[\"bert_input\"][j])\\n        #print(length_)\\n        assert length_==SEQ_LEN, \"sequence size is not \"+str(SEQ_LEN)+\": \"+ str(length_)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is sequence length fixed?\n",
    "\"\"\"for i in range(1,1000):\n",
    "    batch = next(iter(dl))\n",
    "    for j in range(1,2): # batchsize\n",
    "        length_ = len(batch[\"bert_input\"][j])\n",
    "        #print(length_)\n",
    "        assert length_==SEQ_LEN, \"sequence size is not \"+str(SEQ_LEN)+\": \"+ str(length_)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feee7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2788, 1010, 2002, 2052, 2022, 13311, 2105, 1996, 2542, 2282, 1010, 2652, 103, 2010, 10899, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2007, 0, 0, 0]\n",
      "[2002, 1005, 1040, 2464, 1996, 103, 2471, 2011, 6707, 103, 6195, 103, 103, 1037, 2210, 2402, 2005, 1996, 103, 9476, 1010, 2021, 2007, 3080, 103, 1010, 25294, 2007, 2014, 3428, 103, 6701, 2001, 2411, 103, 2000, 2477, 2008, 2020, 3080, 1012]\n",
      "[0, 0, 0, 0, 0, 3185, 0, 0, 0, 1010, 0, 2002, 2001, 0, 0, 0, 2005, 0, 18720, 0, 0, 0, 0, 0, 12334, 0, 2247, 0, 0, 0, 1010, 0, 0, 0, 6086, 0, 0, 0, 0, 0, 0]\n",
      "[103, 2074, 103, 2298, 2012, 103, 7163, 2239, 2741, 2032, 8134, 4937, 22436, 2594, 1012]\n",
      "[2021, 0, 2028, 0, 0, 1037, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2008, 2018, 6793, 12756, 1005, 1055, 2933, 2043, 2016, 2288, 10839, 5102, 103, 1012]\n",
      "[0, 0, 2042, 0, 0, 0, 0, 0, 0, 0, 2032, 0, 3041, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[  101,  2788,  1010,  2002,  2052,  2022, 13311,  2105,  1996,  2542,\n",
       "           2282,  1010,  2652,   103,  2010, 10899,  1012,   102,  2002,  1005,\n",
       "           1040,  2464,  1996,   103,  2471,  2011,  6707,   103,  6195,   103,\n",
       "            103,  1037,  2210,  2402,  2005,  1996,   103,  9476,  1010,  2021,\n",
       "           2007,  3080,   103,  1010, 25294,  2007,  2014,  3428,   103,  6701,\n",
       "           2001,  2411,   103,  2000,  2477,  2008,  2020,  3080,  1012,   102,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,   103,  2074,   103,  2298,  2012,   103,  7163,  2239,  2741,\n",
       "           2032,  8134,  4937, 22436,  2594,  1012,   102,  2008,  2018,  6793,\n",
       "          12756,  1005,  1055,  2933,  2043,  2016,  2288, 10839,  5102,   103,\n",
       "           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'label': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,  2007,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,  3185,     0,     0,     0,  1010,     0,  2002,\n",
       "           2001,     0,     0,     0,  2005,     0, 18720,     0,     0,     0,\n",
       "              0,     0, 12334,     0,  2247,     0,     0,     0,  1010,     0,\n",
       "              0,     0,  6086,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [    0,  2021,     0,  2028,     0,     0,  1037,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,  2042,\n",
       "              0,     0,     0,     0,     0,     0,     0,  2032,     0,  3041,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'segment': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'subseq': tensor([0, 1])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6b813",
   "metadata": {},
   "source": [
    "#### Visualize encoded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44515720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] usually, he would be tearing around the living room, playing [MASK] his toys. [SEP] he'd seen the [MASK] almost by mistake [MASK] considering [MASK] [MASK] a little young for the [MASK] cartoon, but with older [MASK], argus with her brothers [MASK] mason was often [MASK] to things that were older. [SEP] [PAD] [PAD] [PAD] [PAD] [CLS] [MASK] just [MASK] look at [MASK] minion sent him practically catatonic. [SEP] that had supporters megan's plan when she got hurling dressed [MASK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*((batch[\"input\"]))))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828f7d6",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "Positional Embedding (see To DO) must be altered otherwise use nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc6b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, embed_size, seq_len):\n",
    "        super().__init__()\n",
    "        n = 10000 # scalar for pos encoding\n",
    "        # create embedding matrix dim(seq_len  x embed_size)\n",
    "        self.embed_matrix = torch.zeros(seq_len, embed_size).float()\n",
    "        # positional encoding not to be updated while gradient descent\n",
    "        self.embed_matrix.require_grad = False\n",
    "        \n",
    "        # compute embedding for each position in input\n",
    "        for position in range(seq_len):\n",
    "            # run trough every component of embedding vector for each position with stride 2\n",
    "            for c in range(0, embed_size, 2):\n",
    "                # even \n",
    "                self.embed_matrix[position,c] = math.sin(position/(n**(2*c/embed_size)))\n",
    "                # uneven\n",
    "                self.embed_matrix[position,c+1] = math.cos(position/(n**(2*c/embed_size)))\n",
    "        \n",
    "        # self.embed_matrix =  embed_matrix.unsqueeze(0) \n",
    "    def forward(self, x):\n",
    "        return self.embed_matrix\n",
    "            \n",
    "\n",
    "class BERTEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len=SEQ_LEN, n_segments=N_SEGMENTS, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        # token embedding: transforms (vocabulary size, number of tokens) into (vocabulary size, number of tokens, length of embdding vector)\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0) # padding remains 0 during training\n",
    "        # segment embedding for sentence 1, sentence 2, padding\n",
    "        self.segment = nn.Embedding(n_segments, embed_size, padding_idx=0)\n",
    "        # embedding of position\n",
    "        self.position = PositionEmbedding(embed_size, seq_len) \n",
    "        # droput probability per token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, sequence, seg_label):\n",
    "        return self.dropout(self.token(sequence) + self.segment(seg_label) + self.position(sequence))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75a36f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor([  101,  2788,  1010,  2002,  2052,  2022, 13311,  2105,  1996,  2542,\n",
      "         2282,  1010,  2652,   103,  2010, 10899,  1012,   102,  2002,  1005,\n",
      "         1040,  2464,  1996,   103,  2471,  2011,  6707,   103,  6195,   103,\n",
      "          103,  1037,  2210,  2402,  2005,  1996,   103,  9476,  1010,  2021,\n",
      "         2007,  3080,   103,  1010, 25294,  2007,  2014,  3428,   103,  6701,\n",
      "         2001,  2411,   103,  2000,  2477,  2008,  2020,  3080,  1012,   102,\n",
      "            0,     0,     0,     0])\n",
      "torch.Size([64])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0])\n",
      "torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "# embedding test: tokenized sequence\n",
    "sample_seq = batch['input'][0] \n",
    "sample_seg = batch['segment'][0]\n",
    "print(sample_seq.size())\n",
    "print(sample_seq)\n",
    "print(sample_seg.size())\n",
    "print(sample_seg)\n",
    "\n",
    "bert = BERTEmbedding(VOCAB_SIZE, EMBED_SIZE)\n",
    "\n",
    "batch_embed = bert(batch['input'][0], batch['segment'][0])\n",
    "\n",
    "print(batch_embed.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e006",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Cant be downloaded automatically from huggingface. Needs to be downloaded manually:\n",
    "\n",
    "1) download from kaggle and \n",
    "2) extract in finetuning folder \n",
    "3) Delete the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b2f30c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset jigsaw_toxicity_pred/default to C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-2c7f4622bb3d8449/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions:             To use jigsaw_toxicity_pred you have to download it manually from Kaggle: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n            You can manually download the data from it's homepage or use the Kaggle CLI tool (follow the instructions here: https://www.kaggle.com/docs/api)\n            Please extract all files in one folder and then load the dataset with:\n            `datasets.load_dataset('jigsaw_toxicity_pred', data_dir='/path/to/extracted/data/')`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m toxic_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmorit\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUNI\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMaster\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWS23\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPML\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbert_from_scratch.toxic_comment\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfinetuning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkaggle-toxic_comment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m toxic_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjigsaw_toxicity_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_dir\u001b[38;5;241m=\u001b[39mtoxic_path)\n\u001b[0;32m      3\u001b[0m toxic_dataset\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[38;5;241m=\u001b[39mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1649\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[1;32m-> 1649\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1650\u001b[0m         dl_manager,\n\u001b[0;32m   1651\u001b[0m         verification_mode,\n\u001b[0;32m   1652\u001b[0m         check_duplicate_keys\u001b[38;5;241m=\u001b[39mverification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS,\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs,\n\u001b[0;32m   1655\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:963\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    962\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m--> 963\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m    965\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\jigsaw_toxicity_pred\\9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85\\jigsaw_toxicity_pred.py:82\u001b[0m, in \u001b[0;36mJigsawToxicityPred._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     79\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(dl_manager\u001b[38;5;241m.\u001b[39mmanual_dir))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_dir):\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjigsaw_toxicity_pred\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, data_dir=...)`. Manual download instructions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     87\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mSplitGenerator(\n\u001b[0;32m     88\u001b[0m         name\u001b[38;5;241m=\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTRAIN,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     ),\n\u001b[0;32m    101\u001b[0m ]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment does not exist. Make sure you insert a manual dir via `datasets.load_dataset('jigsaw_toxicity_pred', data_dir=...)`. Manual download instructions:             To use jigsaw_toxicity_pred you have to download it manually from Kaggle: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n            You can manually download the data from it's homepage or use the Kaggle CLI tool (follow the instructions here: https://www.kaggle.com/docs/api)\n            Please extract all files in one folder and then load the dataset with:\n            `datasets.load_dataset('jigsaw_toxicity_pred', data_dir='/path/to/extracted/data/')`"
     ]
    }
   ],
   "source": [
    "toxic_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment\"\n",
    "toxic_dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)\n",
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd0356",
   "metadata": {},
   "source": [
    "#### Test with standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f394a9e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toxic_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m----> 2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(toxic_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloader))\n\u001b[0;32m      4\u001b[0m batch\n",
      "\u001b[1;31mNameError\u001b[0m: name 'toxic_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203ec61",
   "metadata": {},
   "source": [
    "#### Standard Tokenizer not sufficient, padding is missing and probably also truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a7997a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2035, 4961, 2039, 3531, 2644, 3800, 7699, 5815, 14652, 2000, 16948, 1012, 2017, 2031, 2042, 2988, 2000, 16948, 1024, 3158, 9305, 2964, 1999, 5082, 2539, 1024, 2676, 1010, 12022, 2385, 1010, 2384, 1006, 11396, 1007, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch[\"comment_text\"])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc5db146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] all grown up please stop purposefully adding nonsense to wikipedia. you have been reported to wikipedia : vandalism in progress 19 : 27, jun 16, 2005 ( utc ) [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*(encoded_input[\"input_ids\"])))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411158c",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65f9e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComment(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows:int=None):\n",
    "        \n",
    "        if not split in [\"train\",\"test\"]:\n",
    "            raise ValueError(\"Parameter has to be 'train' or 'test'\")\n",
    "            \n",
    "        if n_rows is not None:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path, split=split+\"[0:\"+str(n_rows)+\"]\")#[split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)#[split]\n",
    "        \n",
    "        \n",
    "        self.nrows = len(self.dataset) \n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nrows\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        # Step 1: get row\n",
    "        output = self.dataset[item]\n",
    "        #print(output)\n",
    "        \n",
    "        # Step 2: tokenize comment\n",
    "        output[\"bert_input\"] = tokenizer(\n",
    "            output[\"comment_text\"],\n",
    "            max_length=self.seq_len ,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        output.pop(\"comment_text\") #delete raw text\n",
    "        \n",
    "        # Step 3: add bert_label and segment_label like in pretraining task for consistency TODO: Correct?\n",
    "        output[\"bert_label\"] = torch.zeros(self.seq_len)\n",
    "        output[\"segment_label\"] = torch.ones(self.seq_len)\n",
    "        \n",
    "        # Step 4: collect different labels to one tensor \n",
    "        # TODO: desired?\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def get_sent(self, index): #selfmade\n",
    "        '''gets sentence pair as dicitinary s1, s2, isNext'''\n",
    "        isNext = random.random() > 0.5\n",
    "        \n",
    "        t1 = self.dataset[index][\"text\"]\n",
    "        if isNext:\n",
    "            t2 = self.dataset[index+1][\"text\"]\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            t2 = self.get_random_line(index+1)[\"text\"]\n",
    "            return t1, t2, 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7d0bf",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "12bf7b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ToxicComment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test2 \u001b[38;5;241m=\u001b[39m ToxicComment(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(test2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ToxicComment' is not defined"
     ]
    }
   ],
   "source": [
    "test2 = ToxicComment(tokenizer=tokenizer, seq_len=SEQ_LEN, split = \"train\", n_rows = 100)\n",
    "len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3c0766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'severe_toxic': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'obscene': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'threat': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'insult': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'identity_hate': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'bert_input': tensor([[[  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "           18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "            1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "            3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "            1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "            1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "            6486,  1012, 16327,   102]],\n",
       " \n",
       "         [[  101,  1040,  1005, 22091,  2860,   999,  2002,  3503,  2023,  4281,\n",
       "            6120,  1045,  1005,  1049,  9428,  5881,  2007,  1012,  4283,  1012,\n",
       "            1006,  2831,  1007,  2538,  1024,  4868,  1010,  2254,  2340,  1010,\n",
       "            2355,  1006, 11396,  1007,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  4931,  2158,  1010,  1045,  1005,  1049,  2428,  2025,  2667,\n",
       "            2000, 10086,  2162,  1012,  2009,  1005,  1055,  2074,  2008,  2023,\n",
       "            3124,  2003,  7887,  9268,  7882,  2592,  1998,  3331,  2000,  2033,\n",
       "            2083, 10086,  2015,  2612,  1997,  2026,  2831,  3931,  1012,  2002,\n",
       "            3849,  2000,  2729,  2062,  2055,  1996,  4289,  3436,  2084,  1996,\n",
       "            5025, 18558,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1000,  2062,  1045,  2064,  1005,  1056,  2191,  2151,  2613,\n",
       "           15690,  2006,  7620,  1011,  1045,  4999,  2065,  1996,  2930,  6747,\n",
       "            2323,  2022,  2101,  2006,  1010,  2030,  1037,  4942, 29015,  1997,\n",
       "            1000,  1000,  4127,  1997, 13436,  1000,  1000,  1011,  1045,  2228,\n",
       "            1996,  7604,  2089,  2342, 29369,  2075,  2061,  2008,  2027,  2024,\n",
       "            2035,  1999,  1996,  6635,  2168,  4289, 29464,  3058,  4289,  4385,\n",
       "            1012,  1045,  2064,   102]],\n",
       " \n",
       "         [[  101,  2017,  1010,  2909,  1010,  2024,  2026,  5394,  1012,  2151,\n",
       "            3382,  2017,  3342,  2054,  3931,  2008,  1005,  1055,  2006,  1029,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1000, 23156,  2013,  2033,  2004,  2092,  1010,  2224,  1996,\n",
       "            5906,  2092,  1012,  1087,  2831,  1000,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101, 10338,  6342,  9102,  2077,  2017, 18138,  2105,  2006,  2026,\n",
       "            2147,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2115,  3158,  9305,  2964,  2000,  1996,  4717, 11895,  2099,\n",
       "           21827,  3720,  2038,  2042, 16407,  1012,  3531,  2123,  1005,  1056,\n",
       "            2079,  2009,  2153,  1010,  2030,  2017,  2097,  2022,  7917,  1012,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  3374,  2065,  1996,  2773,  1005, 14652,  1005,  2001,  5805,\n",
       "            2000,  2017,  1012,  4312,  1010,  1045,  1005,  1049,  2025, 16533,\n",
       "            2000,  4339,  2505,  1999,  1996,  3720,  1006, 10166,  2027,  2052,\n",
       "            5376,  2006,  2033,  2005,  3158,  9305,  2964,  1007,  1010,  1045,\n",
       "            1005,  1049,  6414, 17942,  2008,  2009,  2022,  2062,  4372,  5666,\n",
       "           20464, 24174,  2594,  2061,  2028,  2064,  2224,  2009,  2005,  2082,\n",
       "            2004,  1037,  4431,   102]],\n",
       " \n",
       "         [[  101, 12139,  2006,  2023,  3395,  1998,  2029,  2024, 10043,  2000,\n",
       "            2216,  1997,  4241, 15909, 25619,  5004,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]]]),\n",
       " 'bert_label': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'segment_label': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl2 = DataLoader(test2,batch_size=10,shuffle=False)\n",
    "next(iter(dl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeff4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(dl2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82fd79a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(dl))[\"bert_input\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada6b40",
   "metadata": {},
   "source": [
    "## Functions for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82ee2f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class BertTokenizer():\\n    def __init__(self, task_type=\"pretrain\"):\\n        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\\n            raise ValueError(\"task not implemented\")\\n        pass\\n    \\n    def __call__()'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class BertTokenizer():\n",
    "    def __init__(self, task_type=\"pretrain\"):\n",
    "        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\n",
    "            raise ValueError(\"task not implemented\")\n",
    "        pass\n",
    "    \n",
    "    def __call__()\"\"\"\n",
    "# i noticed we dont need any callable class to do transformation on the datasets since everything is handeled by our dataloaders\n",
    "# ie we dont need rescaling etc.\n",
    "# maybe ask supervisor if we need to save back the tokenized text or if it is okay to do it on the fly and leave the load_data transformation parameter at None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bf4a2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "\n",
    "def load_data(dataset:str, transformation=None, n_train:int=None, n_test:int=None): # transformation callable\n",
    "    \n",
    "    if dataset == \"bookcorpus\":\n",
    "        train = Bookcorpus(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        return train, None\n",
    "    \n",
    "    elif dataset == \"jigsaw_toxicity_pred\":\n",
    "        train = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        \n",
    "        test = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"test\",\n",
    "            n_rows=n_test\n",
    "        )\n",
    "        return train, test\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "750c0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a85a9ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'bert_input': tensor([[  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "          18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "           1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "           3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "           1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "           1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "           6486,  1012, 16327,   102]]),\n",
       " 'bert_label': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'segment_label': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c46fb18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'bert_input': tensor([[ 101, 4067, 2017, 2005, 4824, 1012, 1045, 2228, 2200, 3811, 1997, 2017,\n",
       "          1998, 2052, 2025, 7065, 8743, 2302, 6594, 1012,  102,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'bert_label': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'segment_label': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac722bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"bookcorpus\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2762923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1552ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x, outfile:str=None): # can have more args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
