# global constants
TOXIC = r"C:\Users\Johannes\Project Machine Learning\datasets\finetuning\toxic_comment"
SEQ_LEN = 64 # maximum sequence length
VOCAB_SIZE = 30522  # = len(tokenizer.vocab)
N_SEGMENTS = 3 # number of segmentation labels
EMBED_SIZE = 768 # size of embedding vector
DROPOUT = 0.1 # dropout chance
