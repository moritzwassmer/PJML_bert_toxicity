{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97783",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* think about visualizations for text, preprocessing text, etc.\n",
    "* cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64 # maximum sequence length\n",
    "VOCAB_SIZE = 30522  # = len(tokenizer.vocab)\n",
    "N_SEGMENTS = 3 # number of segmentation labels\n",
    "EMBED_SIZE = 768 # size of embedding vector\n",
    "DROPOUT = 0.1 # dropout chance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebacda",
   "metadata": {},
   "source": [
    "#### Tokenizer - use pretrained, at least for prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model_max_length = SEQ_LEN # might not be correct in case of pretraining where we add CLS at the end, check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose an appropriate tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827bc7f",
   "metadata": {},
   "source": [
    "#### Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc5fe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncation_side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5adf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # we might need to fixate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2db1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09c4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a719881",
   "metadata": {},
   "source": [
    "#### Tokenizer example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6afd8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am moritz, who are you ?\"#[\"hi i am moritz\", \"no you are not moritz, you are kevin\"]\n",
    "encoded_input = tokenizer(text)#,padding=True, truncation=True)\n",
    "# , return_tensors='pt') use this for pt tensors\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432fff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f874a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi i am moritz, who are you? [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e006",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Cant be downloaded automatically from huggingface. Needs to be downloaded manually:\n",
    "\n",
    "1) download from kaggle and \n",
    "2) extract in finetuning folder \n",
    "3) Delete the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f30c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2640a7a802c24785a304632ea5b09f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 159571\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 63978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toxic_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment\"\n",
    "toxic_path = r\"C:\\Users\\Johannes\\Project Machine Learning\\datasets\\finetuning\\toxic_comment\"\n",
    "toxic_dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)\n",
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd0356",
   "metadata": {},
   "source": [
    "#### Test with standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f394a9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 159571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comment_text': ['\"\\n\\n sincere reply to Ninlil \\n  Dear Ninhil, I thank you very much for intervening... at least this much is accepted by a neutral arbitrator that in case two authors differ in their versions, both versions must be allowed to be retained.  Since you wanted specific instances, in the book named Ancient India (1956) by K.A.N.Sastri, he has stated clearly while describing Rajendra Chola I that among the list of conquests (at one point he stated clearly thus:) \\'next was the turn Satyasraya who was defeated and banished out of his capital\\'.  This can be contrasted with the statement in this Article that Satyasraya was able to control Chola aggression basing his contention on material from author Suryanath Kamath, without commenting on the accuracy of Suryanath Kamath\\'s version, I accept your verdict that both versions should be allowed to be retained.  Dinesh is also erring badly when he writes subsequently in this article about Jayasimha also fighting hard with the Cholas (in fact Rajendra Chola ruled during the time of Satyasraya as well as Jayasimha and has claimed to defeat both) and also goes on to add that both sides lost and won but both K.A.N.Sastri and Rajendra\\'s inscriptions at the Tanjore temple (courtesy the summary of Prof. Huntzsch, noted epigraphist appointed by the Archaelogical Survey of India, a body of the Ministry of Human Resource Development, Govt. of India, whose finds are given in detail at inscriptions.whatsindia.com/ - a site despised unnecessarily by Dinesh Kannambadi as a POV site because its findings go against \\'historical events and their conclusions\\' of his liking) state that he emulated his father in winning the \\'seven and a half lakshas of Rattapadi\\' and where Prof. Huntzsch has clearly arrived at the conclusion (after reading Chola inscriptions from the times of Raja Raja Chola because it was from his time onwards that there were several wars between the Cholas and Chalukyas) that winning seven and a half lakshas of Rattapadi meant winning the war and levying tribute on the losing Chalukya king.  Satyasraya lost twice, once to Raja Raja Chola when the Chola army was led by his son Rajendra and once to Rajendra, who also went on to defeat Satyasraya\\'s successor Jayasimha II and Prof. Huntzsch once again attests the expression by Rajendra Chola I in the inscription at the Tanjore temple, a World Heritage site \\': \"\"the seven and a half lakshas of Iratta-padi, (which was) strong by nature, (through the conquest of which) immeasurable fame arose,[7] (and which he took from) Jayasimha, who, out of fear (and) full of vengeance, turned his back at Muyangi and hid himself\"\", as the Chola once again winning the war (this time it was over Vengi with the Chola army first routing the Chalukyas near Rajahmundry and subsequently at Masangi (called Muyangi in the Chola inscriptions in Tamil langauge), causing their king Jayasimha to banish and then levying tribute on the defeated Chalukya.\\' These have been attested by both K.A.N.Sastri and R.C.Roychowdhuri in their two books of the same name i.e. Ancient India.  \\n\\nTo repeat the above, the author R.C.Roychowdhuri in his book Ancient India borrows heavily by quoting extensively from K.A.N.Sastri - this unfortunately, goes against the thinking of user Kannambadi who is repeating blindly that I have no sources - I can assure you certainly I can read and write. It is very poor in taste, the expression of Dinesh Kannambadi who has gone on to justify my previous blocking, as is evident on my talk page dear Ninhil, this was done by user Blinguen an admin probably, who barged into my talk page and blocked me without giving any reason the second time i.e. he violated two rules of Wikipedia which are being advocated by Dinesh Kannambadi and another clique at work in wikipedia, first they ask people like us, who draw from sources and post at times historical events which are not palatable to them,  that we should first post on the discussion page, but in my case I was straight away blocked on the first occasion by user Blinguen who certainly did not discuss with me on the article discussion page nor did he discuss the topic itself with me.  The second time Blinguen entered my talk page, left a comment that he did not like what I wrote for he thought what I wrote was a personal commentary (when in actuality, it was a summary of Chola historty from which I drew from Prof. Huntzsch\\'s summary from inscriptions.whatsindia.com/ but had made the mistake (I am not shy of admitting I make mistakes) of not posting the source, but before I realized and re-entered the Chola page, I found first that I was not able to edit and found to my horror that Blinguen has arrived at a certain conclusion and blocked me without giving me a chance to explain, discuss or argue with me on the concerned topic and its contents, just as Dinesh Kannambadi is trying to do now by painting anyone whose findings he doesn\\'t like, in black.\\n\\nMr.Kann'],\n",
       " 'toxic': tensor([0]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([0]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([0]),\n",
       " 'identity_hate': tensor([0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "dataset_length = len(toxic_dataset[\"train\"])\n",
    "print(\"Length of dataset:\", dataset_length)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203ec61",
   "metadata": {},
   "source": [
    "#### Standard Tokenizer not sufficient, padding is missing and probably also truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a7997a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1000, 18006, 7514, 2000, 9152, 20554, 4014, 6203, 9152, 25311, 4014, 1010, 1045, 4067, 2017, 2200, 2172, 2005, 26623, 1012, 1012, 1012, 2012, 2560, 2023, 2172, 2003, 3970, 2011, 1037, 8699, 12098, 16313, 16259, 2008, 1999, 2553, 2048, 6048, 11234, 1999, 2037, 4617, 1010, 2119, 4617, 2442, 2022, 3039, 2000, 2022, 6025, 1012, 2144, 2017, 2359, 3563, 12107, 1010, 1999, 1996, 2338, 2315, 3418, 2634, 1006, 3838, 1007, 2011, 1047, 1012, 1037, 1012, 1050, 1012, 21871, 18886, 1010, 2002, 2038, 3090, 4415, 2096, 7851, 11948, 19524, 16480, 2721, 1045, 2008, 2426, 1996, 2862, 1997, 9187, 2015, 1006, 2012, 2028, 2391, 2002, 3090, 4415, 2947, 1024, 1007, 1005, 2279, 2001, 1996, 2735, 2938, 16303, 29539, 2040, 2001, 3249, 1998, 21319, 2041, 1997, 2010, 3007, 1005, 1012, 2023, 2064, 2022, 22085, 2007, 1996, 4861, 1999, 2023, 3720, 2008, 2938, 16303, 29539, 2001, 2583, 2000, 2491, 16480, 2721, 14974, 6403, 2290, 2010, 18974, 2006, 3430, 2013, 3166, 7505, 16811, 2705, 27829, 8988, 1010, 2302, 15591, 2006, 1996, 10640, 1997, 7505, 16811, 2705, 27829, 8988, 1005, 1055, 2544, 1010, 1045, 5138, 2115, 14392, 2008, 2119, 4617, 2323, 2022, 3039, 2000, 2022, 6025, 1012, 11586, 9953, 2003, 2036, 9413, 4892, 6649, 2043, 2002, 7009, 3525, 1999, 2023, 3720, 2055, 24120, 5332, 2213, 3270, 2036, 3554, 2524, 2007, 1996, 16480, 8523, 1006, 1999, 2755, 11948, 19524, 16480, 2721, 5451, 2076, 1996, 2051, 1997, 2938, 16303, 29539, 2004, 2092, 2004, 24120, 5332, 2213, 3270, 1998, 2038, 3555, 2000, 4154, 2119, 1007, 1998, 2036, 3632, 2006, 2000, 5587, 2008, 2119, 3903, 2439, 1998, 2180, 2021, 2119, 1047, 1012, 1037, 1012, 1050, 1012, 21871, 18886, 1998, 11948, 19524, 1005, 1055, 12920, 2012, 1996, 9092, 5558, 2890, 3379, 1006, 14571, 1996, 12654, 1997, 11268, 1012, 5690, 2480, 11624, 1010, 3264, 4958, 8004, 24342, 2923, 2805, 2011, 1996, 7905, 21147, 20734, 5002, 1997, 2634, 1010, 1037, 2303, 1997, 1996, 3757, 1997, 2529, 7692, 2458, 1010, 22410, 1012, 1997, 2634, 1010, 3005, 4858, 2024, 2445, 1999, 6987, 2012, 12920, 1012, 2054, 11493, 9032, 1012, 4012, 1013, 1011, 1037, 2609, 26626, 4895, 2638, 9623, 22740, 2135, 2011, 11586, 9953, 22827, 13129, 9024, 2072, 2004, 1037, 13433, 2615, 2609, 2138, 2049, 9556, 2175, 2114, 1005, 3439, 2824, 1998, 2037, 15306, 1005, 1997, 2010, 16663, 1007, 2110, 2008, 2002, 7861, 8898, 2010, 2269, 1999, 3045, 1996, 1005, 2698, 1998, 1037, 2431, 2474, 28132, 2015, 1997, 9350, 2696, 15455, 2072, 1005, 1998, 2073, 11268, 1012, 5690, 2480, 11624, 2038, 4415, 3369, 2012, 1996, 7091, 1006, 2044, 3752, 16480, 2721, 12920, 2013, 1996, 2335, 1997, 10164, 10164, 16480, 2721, 2138, 2009, 2001, 2013, 2010, 2051, 9921, 2008, 2045, 2020, 2195, 5233, 2090, 1996, 16480, 8523, 1998, 15775, 7630, 4801, 3022, 1007, 2008, 3045, 2698, 1998, 1037, 2431, 2474, 28132, 2015, 1997, 9350, 2696, 15455, 2072, 3214, 3045, 1996, 2162, 1998, 12767, 2075, 7050, 2006, 1996, 3974, 15775, 7630, 4801, 2050, 2332, 1012, 2938, 16303, 29539, 2439, 3807, 1010, 2320, 2000, 10164, 10164, 16480, 2721, 2043, 1996, 16480, 2721, 2390, 2001, 2419, 2011, 2010, 2365, 11948, 19524, 1998, 2320, 2000, 11948, 19524, 1010, 2040, 2036, 2253, 2006, 2000, 4154, 2938, 16303, 29539, 1005, 1055, 6332, 24120, 5332, 2213, 3270, 2462, 1998, 11268, 1012, 5690, 2480, 11624, 2320, 2153, 2012, 22199, 2015, 1996, 3670, 2011, 11948, 19524, 16480, 2721, 1045, 1999, 1996, 9315, 2012, 1996, 9092, 5558, 2890, 3379, 1010, 1037, 2088, 4348, 2609, 1005, 1024, 1000, 1000, 1996, 2698, 1998, 1037, 2431, 2474, 28132, 2015, 1997, 11209, 5946, 1011, 11687, 2072, 1010, 1006, 2029, 2001, 1007, 2844, 2011, 3267, 1010, 1006, 2083, 1996, 9187, 1997, 2029, 1007, 10047, 4168, 28329, 4476, 10375, 1010, 1031, 1021, 1033, 1006, 1998, 2029, 2002, 2165, 2013, 1007, 24120, 5332, 2213, 3270, 1010, 2040, 1010, 2041, 1997, 3571, 1006, 1998, 1007, 2440, 1997, 14096, 1010, 2357, 2010, 2067, 2012, 14163, 12198, 2072, 1998, 11041, 2370, 1000, 1000, 1010, 2004, 1996, 16480, 2721, 2320, 2153, 3045, 1996, 2162, 1006, 2023, 2051, 2009, 2001, 2058, 2310, 3070, 2072, 2007, 1996, 16480, 2721, 2390, 2034, 16972, 1996, 15775, 7630, 4801, 3022, 2379, 10164, 14227, 8630, 2854, 1998, 3525, 2012, 16137, 5654, 2072, 1006, 2170, 14163, 12198, 2072, 1999, 1996, 16480, 2721, 12920, 1999, 6008, 11374, 4887, 3351, 1007, 1010, 4786, 2037, 2332, 24120, 5332, 2213, 3270, 2000, 7221, 4509, 1998, 2059, 12767, 2075, 7050, 2006, 1996, 3249, 15775, 7630, 4801, 2050, 1012, 1005, 2122, 2031, 2042, 18470, 2011, 2119, 1047, 1012, 1037, 1012, 1050, 1012, 21871, 18886, 1998, 1054, 1012, 1039, 1012, 6060, 9905, 21724, 24572, 2072, 1999, 2037, 2048, 2808, 1997, 1996, 2168, 2171, 1045, 1012, 1041, 1012, 3418, 2634, 1012, 2000, 9377, 1996, 2682, 1010, 1996, 3166, 1054, 1012, 1039, 1012, 6060, 9905, 21724, 24572, 2072, 1999, 2010, 2338, 3418, 2634, 17781, 2015, 4600, 2011, 27394, 8077, 2013, 1047, 1012, 1037, 1012, 1050, 1012, 21871, 18886, 1011, 2023, 6854, 1010, 3632, 2114, 1996, 3241, 1997, 5310, 22827, 13129, 9024, 2072, 2040, 2003, 15192, 25734, 2008, 1045, 2031, 2053, 4216, 1011, 1045, 2064, 14306, 2017, 5121, 1045, 2064, 3191, 1998, 4339, 1012, 2009, 2003, 2200, 3532, 1999, 5510, 1010, 1996, 3670, 1997, 11586, 9953, 22827, 13129, 9024, 2072, 2040, 2038, 2908, 2006, 2000, 16114, 2026, 3025, 10851, 1010, 2004, 2003, 10358, 2006, 2026, 2831, 3931, 6203, 9152, 25311, 4014, 1010, 2023, 2001, 2589, 2011, 5310, 1038, 2989, 24997, 2019, 4748, 10020, 2763, 1010, 2040, 19398, 2094, 2046, 2026, 2831, 3931, 1998, 8534, 2033, 2302, 3228, 2151, 3114, 1996, 2117, 2051, 1045, 1012, 1041, 1012, 2002, 14424, 2048, 3513, 1997, 16948, 2029, 2024, 2108, 11886, 2011, 11586, 9953, 22827, 13129, 9024, 2072, 1998, 2178, 18856, 7413, 2012, 2147, 1999, 16948, 1010, 2034, 2027, 3198, 2111, 2066, 2149, 1010, 2040, 4009, 2013, 4216, 1998, 2695, 2012, 2335, 3439, 2824, 2029, 2024, 2025, 14412, 27892, 2000, 2068, 1010, 2008, 2057, 2323, 2034, 2695, 2006, 1996, 6594, 3931, 1010, 2021, 1999, 2026, 2553, 1045, 2001, 3442, 2185, 8534, 2006, 1996, 2034, 6686, 2011, 5310, 1038, 2989, 24997, 2040, 5121, 2106, 2025, 6848, 2007, 2033, 2006, 1996, 3720, 6594, 3931, 4496, 2106, 2002, 6848, 1996, 8476, 2993, 2007, 2033, 1012, 1996, 2117, 2051, 1038, 2989, 24997, 3133, 2026, 2831, 3931, 1010, 2187, 1037, 7615, 2008, 2002, 2106, 2025, 2066, 2054, 1045, 2626, 2005, 2002, 2245, 2054, 1045, 2626, 2001, 1037, 3167, 8570, 1006, 2043, 1999, 5025, 3012, 1010, 2009, 2001, 1037, 12654, 1997, 16480, 2721, 2010, 25485, 2100, 2013, 2029, 1045, 3881, 2013, 11268, 1012, 5690, 2480, 11624, 1005, 1055, 12654, 2013, 12920, 1012, 2054, 11493, 9032, 1012, 4012, 1013, 2021, 2018, 2081, 1996, 6707, 1006, 1045, 2572, 2025, 11004, 1997, 17927, 1045, 2191, 12051, 1007, 1997, 2025, 14739, 1996, 3120, 1010, 2021, 2077, 1045, 3651, 1998, 2128, 1011, 3133, 1996, 16480, 2721, 3931, 1010, 1045, 2179, 2034, 2008, 1045, 2001, 2025, 2583, 2000, 10086, 1998, 2179, 2000, 2026, 5469, 2008, 1038, 2989, 24997, 2038, 3369, 2012, 1037, 3056, 7091, 1998, 8534, 2033, 2302, 3228, 2033, 1037, 3382, 2000, 4863, 1010, 6848, 2030, 7475, 2007, 2033, 2006, 1996, 4986, 8476, 1998, 2049, 8417, 1010, 2074, 2004, 11586, 9953, 22827, 13129, 9024, 2072, 2003, 2667, 2000, 2079, 2085, 2011, 4169, 3087, 3005, 9556, 2002, 2987, 1005, 1056, 2066, 1010, 1999, 2304, 1012, 2720, 1012, 22827, 2078, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch[\"comment_text\"])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc5db146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] \" sincere reply to ninlil dear ninhil, i thank you very much for intervening... at least this much is accepted by a neutral arbitrator that in case two authors differ in their versions, both versions must be allowed to be retained. since you wanted specific instances, in the book named ancient india ( 1956 ) by k. a. n. sastri, he has stated clearly while describing rajendra chola i that among the list of conquests ( at one point he stated clearly thus : )\\'next was the turn satyasraya who was defeated and banished out of his capital \\'. this can be contrasted with the statement in this article that satyasraya was able to control chola aggression basing his contention on material from author suryanath kamath, without commenting on the accuracy of suryanath kamath\\'s version, i accept your verdict that both versions should be allowed to be retained. dinesh is also erring badly when he writes subsequently in this article about jayasimha also fighting hard with the cholas ( in fact rajendra chola ruled during the time of satyasraya as well as jayasimha and has claimed to defeat both ) and also goes on to add that both sides lost and won but both k. a. n. sastri and rajendra\\'s inscriptions at the tanjore temple ( courtesy the summary of prof. huntzsch, noted epigraphist appointed by the archaelogical survey of india, a body of the ministry of human resource development, govt. of india, whose finds are given in detail at inscriptions. whatsindia. com / - a site despised unnecessarily by dinesh kannambadi as a pov site because its findings go against\\'historical events and their conclusions\\'of his liking ) state that he emulated his father in winning the\\'seven and a half lakshas of rattapadi\\'and where prof. huntzsch has clearly arrived at the conclusion ( after reading chola inscriptions from the times of raja raja chola because it was from his time onwards that there were several wars between the cholas and chalukyas ) that winning seven and a half lakshas of rattapadi meant winning the war and levying tribute on the losing chalukya king. satyasraya lost twice, once to raja raja chola when the chola army was led by his son rajendra and once to rajendra, who also went on to defeat satyasraya\\'s successor jayasimha ii and prof. huntzsch once again attests the expression by rajendra chola i in the inscription at the tanjore temple, a world heritage site\\': \" \" the seven and a half lakshas of iratta - padi, ( which was ) strong by nature, ( through the conquest of which ) immeasurable fame arose, [ 7 ] ( and which he took from ) jayasimha, who, out of fear ( and ) full of vengeance, turned his back at muyangi and hid himself \" \", as the chola once again winning the war ( this time it was over vengi with the chola army first routing the chalukyas near rajahmundry and subsequently at masangi ( called muyangi in the chola inscriptions in tamil langauge ), causing their king jayasimha to banish and then levying tribute on the defeated chalukya.\\'these have been attested by both k. a. n. sastri and r. c. roychowdhuri in their two books of the same name i. e. ancient india. to repeat the above, the author r. c. roychowdhuri in his book ancient india borrows heavily by quoting extensively from k. a. n. sastri - this unfortunately, goes against the thinking of user kannambadi who is repeating blindly that i have no sources - i can assure you certainly i can read and write. it is very poor in taste, the expression of dinesh kannambadi who has gone on to justify my previous blocking, as is evident on my talk page dear ninhil, this was done by user blinguen an admin probably, who barged into my talk page and blocked me without giving any reason the second time i. e. he violated two rules of wikipedia which are being advocated by dinesh kannambadi and another clique at work in wikipedia, first they ask people like us, who draw from sources and post at times historical events which are not palatable to them, that we should first post on the discussion page, but in my case i was straight away blocked on the first occasion by user blinguen who certainly did not discuss with me on the article discussion page nor did he discuss the topic itself with me. the second time blinguen entered my talk page, left a comment that he did not like what i wrote for he thought what i wrote was a personal commentary ( when in actuality, it was a summary of chola historty from which i drew from prof. huntzsch\\'s summary from inscriptions. whatsindia. com / but had made the mistake ( i am not shy of admitting i make mistakes ) of not posting the source, but before i realized and re - entered the chola page, i found first that i was not able to edit and found to my horror that blinguen has arrived at a certain conclusion and blocked me without giving me a chance to explain, discuss or argue with me on the concerned topic and its contents, just as dinesh kannambadi is trying to do now by painting anyone whose findings he doesn\\'t like, in black. mr. kann [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*(encoded_input[\"input_ids\"])))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411158c",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65f9e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComment(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=SEQ_LEN, split=\"train\", n_rows:int=None):\n",
    "        \n",
    "        if not split in [\"train\",\"test\"]:\n",
    "            raise ValueError(\"Parameter has to be 'train' or 'test'\")  \n",
    "        \n",
    "        self.n_rows = n_rows\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "            \n",
    "        if self.n_rows is not None:\n",
    "            n_rows_str = f\"[0:{self.n_rows}]\" if self.n_rows is not None else \"\"\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path, split=f\"{self.split}{n_rows_str}\")#[split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)#[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        # Step 1: get row\n",
    "        output = self.dataset[item]\n",
    "\n",
    "        # Step 2: tokenize comment\n",
    "        output[\"input\"] = self.tokenizer(\n",
    "            output[\"comment_text\"],\n",
    "            max_length=self.seq_len ,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # flatten output\n",
    "        output[\"input\"] = output[\"input\"].squeeze()\n",
    "        \n",
    "        output.pop(\"comment_text\") #delete raw text\n",
    "        \n",
    "        # Step 3: add segment_label like in pretraining task for consistency \n",
    "        output[\"segment\"] = torch.ones(self.seq_len)\n",
    "        \n",
    "        # Step 4: collect different labels to one tensor \n",
    "        labels = torch.cat([output[key] if isinstance(output[key], torch.Tensor) else torch.tensor([output[key]]) for key in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],dim=-1)\n",
    "        output[\"labels\"] = labels\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7d0bf",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12bf7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = ToxicComment(tokenizer=tokenizer, seq_len=SEQ_LEN, split = \"train\", n_rows = 100)\n",
    "len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3c0766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': tensor([0]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([0]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([0]),\n",
       " 'identity_hate': tensor([0]),\n",
       " 'input': tensor([[  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "          18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "           1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "           3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "           1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "           1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "           6486,  1012, 16327,   102]]),\n",
       " 'segment': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " 'labels': tensor([[0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl2 = DataLoader(test2,batch_size=1,shuffle=False)\n",
    "batch = next(iter(dl2))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeff4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82fd79a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"input\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed0852",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fc6b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, embed_size, seq_len):\n",
    "        super().__init__()\n",
    "        n = 10000 # scalar for pos encoding\n",
    "        # create embedding matrix dim(seq_len  x embed_size)\n",
    "        self.embed_matrix = torch.zeros(seq_len, embed_size).float()\n",
    "        # positional encoding not to be updated while gradient descent\n",
    "        self.embed_matrix.require_grad = False\n",
    "        \n",
    "        # compute embedding for each position in input\n",
    "        for position in range(seq_len):\n",
    "            # run trough every component of embedding vector for each position with stride 2\n",
    "            for c in range(0, embed_size, 2):\n",
    "                # even \n",
    "                self.embed_matrix[position,c] = math.sin(position/(n**(2*c/embed_size)))\n",
    "                # uneven\n",
    "                self.embed_matrix[position,c+1] = math.cos(position/(n**(2*c/embed_size)))\n",
    "        \n",
    "        # self.embed_matrix =  embed_matrix.unsqueeze(0) \n",
    "    def forward(self, x):\n",
    "        return self.embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ece658c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embed_size=EMBED_SIZE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        # token embedding: transforms (vocabulary size, number of tokens) into (vocabulary size, number of tokens, length of embdding vector)\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0) # padding remains 0 during training\n",
    "        # embedding of position\n",
    "        self.position = PositionEmbedding(embed_size, seq_len) \n",
    "        # droput probability per token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        return self.dropout(self.token(sequence) + self.position(sequence))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42c4a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_seq size torch.Size([64])\n",
      "tensor([  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
      "        18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
      "         1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
      "         3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
      "         1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
      "         1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
      "         6486,  1012, 16327,   102])\n",
      "torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "# embedding test: tokenized sequence\n",
    "sample_seq = batch['input'][0] \n",
    "print(f'sample_seq size {sample_seq.size()}')\n",
    "print(sample_seq)\n",
    "\n",
    "bert = BERTEmbedding(VOCAB_SIZE, SEQ_LEN)\n",
    "\n",
    "batch_embed = bert(batch['input'][0].long())\n",
    "\n",
    "print(batch_embed.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107853a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "759b5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention heads\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, number_heads, model_dimension):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # model dimension must be divideable into equal parts for the attention heads\n",
    "        assert model_dimension%number_heads == 0\n",
    "        self.number_heads = number_heads\n",
    "        self.att_head_dim = int(model_dimension/number_heads)\n",
    "        \n",
    "        # attention mechanism: query, key, value are linear embeddings -> embedding matrix dim: (model_dimension x model_dimension)\n",
    "        self.query = nn.Linear(model_dimension, model_dimension)\n",
    "        self.key = nn.Linear(model_dimension, model_dimension)\n",
    "        self.value = nn.Linear(model_dimension, model_dimension)\n",
    "        self.lin_output = nn.Linear(model_dimension, model_dimension)\n",
    "    \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \n",
    "        # output dim (batch_size x seq_len x model_dimension) \n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value) \n",
    "        \n",
    "        # transform q,k,v to fit attention heads:(batch_size x seq_len x model_dimension) -> (batch_size x number_heads x seq_len x att_head_dim)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.number_heads, self.att_head_dim)\n",
    "        query = query.permute(0,2,1,3)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.number_heads, self.att_head_dim)\n",
    "        key = key.permute(0,2,1,3)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.number_heads, self.att_head_dim)\n",
    "        value = value.permute(0,2,1,3)\n",
    "        \n",
    "        # calculate dot product between each query and each key and normaliz the output, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score = torch.matmul(query, key.permute(0, 1, 3, 2)) \n",
    "        score_n = score / math.sqrt(self.att_head_dim) # normalize: <q,k>/sqrt(d_k)\n",
    "        \n",
    "        # mask 0 with -infinity so it becomes 0 after softmax, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score_m = score_n.masked_fill(mask == 0, -1e10)    \n",
    "        \n",
    "        # softmax scores along each query, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score_w = nn.functional.softmax(score_m, dim=-1) \n",
    "        \n",
    "        # multiply with value matrix: output weighted sum for each query, output dim: (batch_size x number_heads x seq_len x att_head_dim)\n",
    "        weighted_sum = torch.matmul(score_w, value)\n",
    "        \n",
    "        # concatenate attention heads to 1 output, output dim: (batch_size x number_heads x model_dimension)\n",
    "        weighted_sum = weighted_sum.permute(0, 2, 1, 3).contiguous().view(weighted_sum.shape[0], -1, self.number_heads * self.att_head_dim)\n",
    "        \n",
    "        # linear embedding for output\n",
    "        out = self.lin_output(weighted_sum)      \n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c977081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, hidden_dimension):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        \n",
    "        # linear layer\n",
    "        self.linear1 = nn.Linear(model_dimension, hidden_dimension)\n",
    "        self.linear2 = nn.Linear(hidden_dimension, model_dimension)\n",
    "        # non-linearity\n",
    "        self.non_linear = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.non_linear(self.linear1(x)))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "870b484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder stacks together all the previous modules\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_dimension=EMBED_SIZE, number_heads=12, ff_hidden_dim=EMBED_SIZE*4):\n",
    "        super(Encoder, self).__init__()\n",
    "        # attention heads\n",
    "        self.multihead_attention = MultiHeadAttention (number_heads, model_dimension)\n",
    "        # normalisation layer\n",
    "        self.normlayer = nn.LayerNorm(model_dimension)\n",
    "        self.feedforward_layer = FeedForwardLayer(model_dimension, hidden_dimension=ff_hidden_dim)\n",
    "    \n",
    "    # also residuals possible here\n",
    "    def forward(self, x, mask):\n",
    "        # input x 3x to generate query, key, value\n",
    "        x = self.normlayer(self.multihead_attention(x, x, x, mask))\n",
    "        return self.normlayer(self.feedforward_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82f4474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class for BERT\n",
    "class BERTBase(nn.Module):\n",
    "    # __init__ function takes hyperparameters, initializes the model accordingly and sets up trainable parameters\n",
    "    def __init__(self, vocab_size, model_dimension, number_layers, number_heads):\n",
    "        super().__init__()\n",
    "        self.model_dimension=model_dimension\n",
    "        self.number_layers=number_layers\n",
    "        self.number_heads=number_heads\n",
    "        # hidden layer dimenion of FF is 4*model_dimension (see paper)\n",
    "        self.ff_hidden_layer = 4*model_dimension\n",
    "        # embedding of input \n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, seq_len=SEQ_LEN, embed_size=model_dimension)\n",
    "        # stack encoders\n",
    "        self.encoders = torch.nn.ModuleList() # create empty module list\n",
    "        for _ in range(self.number_layers):\n",
    "            self.encoders.append(Encoder(model_dimension=model_dimension, number_heads=number_heads, ff_hidden_dim=4*model_dimension))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # mask to mark the padded (0) tokens\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1,x.size(1),1).unsqueeze(1)\n",
    "        x = self.embedding(x) \n",
    "        # run trough encoders\n",
    "        for encoder in self.encoders:\n",
    "            x =encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33c67f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning\n",
    "class ToxicityPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    class to predict multivariate class of toxicity\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_out):\n",
    "        super().__init__()\n",
    "        self.tox_classes = 6 # there are 6 classes of toxicity in the dataset\n",
    "        self.linear = nn.Linear(bert_out, self.tox_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # recieve output dimension (batch_size, self.tox_classes)\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66999e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class according to Milestone 1 task sheet\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_dimension, number_layers=12, number_heads=12):\n",
    "        super().__init__()\n",
    "        # base BERT model\n",
    "        self.base_model = BERTBase(vocab_size, model_dimension, number_layers, number_heads)\n",
    "        # toxic comment classfication layer\n",
    "        self.toxic_comment = ToxicityPrediction(self.base_model.model_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return self.toxic_comment(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0c699",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afb04877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainBERT:\n",
    "    def __init__(self, model, train_dataloader, epochs, test_dataloader=None, learning_rate=0.001, threshold=0.5, device='cuda'):\n",
    "        \n",
    "        # hyperparameters for optimization\n",
    "        self.device = device\n",
    "        self.bar = None\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.training_data = train_dataloader\n",
    "        self.testing_data = test_dataloader\n",
    "\n",
    "        # optimizer: Adam\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # learning rate scheduler\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # cost function cross entropy loss for predicting classes of toxicity\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # predictions threshold above which predictions are set True\n",
    "        self.threshold = threshold \n",
    "        \n",
    "        # run training\n",
    "        for epoch in range(self.epochs):\n",
    "            self.training(epoch)\n",
    "\n",
    "    def training(self, epoch):\n",
    "        # init stats\n",
    "        avg_loss = 0.0\n",
    "        corrects_sum = 0\n",
    "        trues_sum = 0\n",
    "        \n",
    "        # set back progress bar\n",
    "        self.bar = None\n",
    "        # create new progress bar\n",
    "        self.bar = tqdm(total=len(self.training_data.dataset), desc=f'Training epoch {epoch+1}', leave=True, position=0)\n",
    "\n",
    "        for i, data in enumerate(self.training_data):\n",
    "            \n",
    "            # send data to GPU/CPU\n",
    "            data ={key: value.to(self.device) for key, value in data.items()}\n",
    "            \n",
    "            # labels convert to float()\n",
    "            labels = data['labels'].float()\n",
    "            \n",
    "            # forward pass: comments trough model\n",
    "            output = self.model.forward(data['input'])\n",
    "            \n",
    "            # compute loss with labels (input, target)\n",
    "            loss = self.criterion(output, labels)\n",
    "            \n",
    "            # backward pass for training\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # average loss per batch\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            # compute accuracy \n",
    "            # softmax the output vector to get probabilites\n",
    "            predictions = nn.functional.softmax(output, dim=1)\n",
    "            # use threshold to determine which of the outputs are considered True\n",
    "            predictions = torch.ge(predictions, self.threshold).int()\n",
    "            # compare with the label and count correct classifications\n",
    "            corrects_sum += (predictions == labels).sum().item()\n",
    "            # sump up total number of Trues in labels for batch\n",
    "            trues_sum += labels.nelement()\n",
    "            \n",
    "            # update progress bar\n",
    "            self.bar.update(self.training_data.batch_size)\n",
    "        \n",
    "        # update learning rate scheduler\n",
    "        self.scheduler.step() \n",
    "        # print stats\n",
    "        print(f'Trainig epoch: {epoch+1}\\nAvg. training loss: {(avg_loss / len(self.training_data)):.2f}\\nAccuracy: {(corrects_sum * 100.0 / trues_sum):.2f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2a6901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n",
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d075fd8d9ce54545a2ac8c6e2753e3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1:   0%|                                                                        | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m bert_trainer \u001b[38;5;241m=\u001b[39m TrainBERT(bert, train_loader, epochs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 25\u001b[0m, in \u001b[0;36mTrainBERT.__init__\u001b[1;34m(self, model, train_dataloader, epochs, test_dataloader, learning_rate, threshold, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# run training\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining(epoch)\n",
      "Cell \u001b[1;32mIn[31], line 54\u001b[0m, in \u001b[0;36mTrainBERT.training\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# backward pass for training\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# average loss per batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training test\n",
    "# set up tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# load test dataset (load_data() is down below -> needs to run first)\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=100, n_test=None)\n",
    "\n",
    "# set up dataloader\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "\n",
    "# set up BERT model\n",
    "bert = Model(vocab_size=VOCAB_SIZE, model_dimension=EMBED_SIZE, number_layers=12, number_heads=12)\n",
    "\n",
    "# number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# train model\n",
    "bert_trainer = TrainBERT(bert, train_loader, epochs, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632f5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n",
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcca38b18e344975aa230b5b5775e283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1:   0%|                                                                     | 0/159571 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training (for cluster)\n",
    "# set up tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# load the entire training data (length 10000) into train\n",
    "train, _ = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=dataset_length, n_test=None)\n",
    "\n",
    "# set up dataloader\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
    "\n",
    "# set up BERT model\n",
    "bert_lm = Model(vocab_size=VOCAB_SIZE, model_dimension=EMBED_SIZE, number_layers=12, number_heads=12)\n",
    "\n",
    "# number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# train model (device to be updated according to cluster GPU)\n",
    "bert_trainer = TrainBERT(bert_lm, train_loader, epochs, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada6b40",
   "metadata": {},
   "source": [
    "## Functions for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class BertTokenizer():\n",
    "    def __init__(self, task_type=\"pretrain\"):\n",
    "        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\n",
    "            raise ValueError(\"task not implemented\")\n",
    "        pass\n",
    "    \n",
    "    def __call__()\"\"\"\n",
    "# i noticed we dont need any callable class to do transformation on the datasets since everything is handeled by our dataloaders\n",
    "# ie we dont need rescaling etc.\n",
    "# maybe ask supervisor if we need to save back the tokenized text or if it is okay to do it on the fly and leave the load_data transformation parameter at None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf4a2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "def load_data(dataset:str, transformation=None, n_train:int=None, n_test:int=None): # transformation callable\n",
    "    \n",
    "    if dataset == \"bookcorpus\":\n",
    "        train = Bookcorpus(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        return train, None\n",
    "    \n",
    "    elif dataset == \"jigsaw_toxicity_pred\":\n",
    "        train = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        \n",
    "        test = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"test\",\n",
    "            n_rows=n_test\n",
    "        )\n",
    "        return train, test\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "750c0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a85a9ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'input': tensor([  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "         18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "          1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "          3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "          1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "          1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "          6486,  1012, 16327,   102]),\n",
       " 'segment': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c46fb18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'input': tensor([ 101, 4067, 2017, 2005, 4824, 1012, 1045, 2228, 2200, 3811, 1997, 2017,\n",
       "         1998, 2052, 2025, 7065, 8743, 2302, 6594, 1012,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'segment': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ac722bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"bookcorpus\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2762923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c1552ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x, outfile:str=None): # can have more args\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
