{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97783",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Ensure that during pre training, both sentences fit into the model at the same time -> DONE but not teted\n",
    "* think about visualizations for text, preprocessing text, etc.\n",
    "* implement interfaces of the task sheet\n",
    "* cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464fe5e",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc737c1",
   "metadata": {},
   "source": [
    "### bookcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b594b1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morit\\anaconda3\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 74004228\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Download + load data from cache or online AUTOMATICALLY\n",
    "# https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bookcorpus\") # alternative, less size datasets.load_dataset(\"bookcorpus\", split=\"train[:10%]\")\n",
    "# split=\"train[10:20]\")\n",
    "# saved here on windows C:\\Users\\morit\\.cache\\huggingface\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f56493",
   "metadata": {},
   "source": [
    "#### Saving huggingface Dataset on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a6ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual save to disk\n",
    "\n",
    "#folder_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\pretraining\"\n",
    "#full_path = folder_path+r\"\\bookcorpus\"\n",
    "\n",
    "#dataset.save_to_disk(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cfd29",
   "metadata": {},
   "source": [
    "#### Loading hf dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a77b037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manual load from disk\n",
    "\n",
    "#dataset = datasets.load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4604f83",
   "metadata": {},
   "source": [
    "#### slicing hf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14821a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][4][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c4a55",
   "metadata": {},
   "source": [
    "#### Standard dataloader - not sufficient we need tokenized output -> implement own dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15cb2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['usually , he would be tearing around the living room , playing with his toys .',\n",
       "  'but just one look at a minion sent him practically catatonic .']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset[\"train\"], batch_size=2)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebacda",
   "metadata": {},
   "source": [
    "#### Tokenizer - use pretrained, at least for prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose an appropriate tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model_max_length = SEQ_LEN # might not be correct in case of pretraining where we add CLS at the end, check that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea8c72",
   "metadata": {},
   "source": [
    "#### Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc5fe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncation_side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5adf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # we might need to fixate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7968bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29eeae0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd494d",
   "metadata": {},
   "source": [
    "#### Tokenizer example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6afd8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am moritz, who are you ?\"#[\"hi i am moritz\", \"no you are not moritz, you are kevin\"]\n",
    "encoded_input = tokenizer(text)#,padding=True, truncation=True)\n",
    "# , return_tensors='pt') use this for pt tensors\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432fff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f874a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi i am moritz, who are you? [SEP]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b4d7e",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca75d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = None\n",
    "n_rows is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8564c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "class Bookcorpus(Dataset): # TODO rewrite \n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "        \"\"\"\n",
    "        n_rows None means take the whole dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        if not split in [\"train\"]:\n",
    "            raise ValueError(\"For Bookcorpus there is only a train split\")\n",
    "            \n",
    "        if n_rows is not None:\n",
    "            self.dataset = load_dataset(\"bookcorpus\", split=split+\"[0:\"+str(n_rows)+\"]\")#[split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"bookcorpus\")#[split]\n",
    "            \n",
    "        self.n_rows = len(self.dataset) \n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rows\n",
    "\n",
    "    def __getitem__(self, item): # TODO Where is truncation if sequence is to long? How is ensured that both sentences fit into the sequence?\n",
    "        \n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        s1, s2, is_next_label = self.get_sent(item)\n",
    "        \n",
    "        # Step 2: replace random words in EACH sentence with mask / random words # copied \n",
    "        t1_random, t1_label = self.random_word(s1)\n",
    "        t2_random, t2_label = self.random_word(s2)\n",
    "        \n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences # copied \n",
    "         # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input # copied \n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "        \n",
    "        \n",
    "        \n",
    "        #return  {\"s1\":s1, \"s2\":s2, \"is_next_label\":is_next_label}\n",
    "        #return {\"t1_random\":t1_random, \"t1_label\":t1_label, \"t2_random\":t2_random, \"t2_label\":t2_label}\n",
    "    \n",
    "    def get_sent(self, index): #selfmade\n",
    "        '''gets sentence pair as dicitinary s1, s2, isNext'''\n",
    "        isNext = random.random() > 0.5\n",
    "        \n",
    "        t1 = self.dataset[index][\"text\"]\n",
    "        if isNext:\n",
    "            t2 = self.dataset[index+1][\"text\"]\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            t2 = self.get_random_line(index+1)[\"text\"]\n",
    "            return t1, t2, 0\n",
    "        \n",
    "    def get_random_line(self, excludedIndex): #selfmade\n",
    "        '''return random single sentence excluding'''\n",
    "        randIndex = random.randint(1, self.__len__())\n",
    "            \n",
    "        # ensure that randIndex is not next sentence\n",
    "        while randIndex == excludedIndex:\n",
    "            randIndex = random.randint(1, self.__len__())\n",
    "        \n",
    "        return self.dataset[randIndex]\n",
    "\n",
    "    def random_word(self, sentence): #copied\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "\n",
    "            # remove cls and sep token\n",
    "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        #assert len(output) == self.seq_len, \"sequence length not fixed! \"+str(len(output)) # from moritz\n",
    "        return output, output_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e21d7",
   "metadata": {},
   "source": [
    "#### Testing the Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40334c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Bookcorpus(tokenizer, n_rows = 100)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b123188",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(test,batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ed0ac1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(1,1000):\\n    batch = next(iter(dl))\\n    for j in range(1,2): # batchsize\\n        length_ = len(batch[\"bert_input\"][j])\\n        #print(length_)\\n        assert length_==SEQ_LEN, \"sequence size is not \"+str(SEQ_LEN)+\": \"+ str(length_)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is sewuence length fixed?\n",
    "\"\"\"for i in range(1,1000):\n",
    "    batch = next(iter(dl))\n",
    "    for j in range(1,2): # batchsize\n",
    "        length_ = len(batch[\"bert_input\"][j])\n",
    "        #print(length_)\n",
    "        assert length_==SEQ_LEN, \"sequence size is not \"+str(SEQ_LEN)+\": \"+ str(length_)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feee7d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[  101,  2788,  1010,  2002,  2052,  2022, 13311,  2105,  1996,  2542,\n",
       "           2282,  1010,  2652,   103,  2010, 10899,  1012,   102,  2044,  2008,\n",
       "            103,  2002,  2018, 18397,   694,  2686,  2042,  4699,  1999,  2151,\n",
       "           1997,  1996,  4620,   103, 22028, 12756,  2741,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2021,  2074,  2028,  2298,  2012,   103,  7163,  2239,  2741,\n",
       "           2032,  8134,  4937, 22436,  2594,  1012,   102,  2016,  2001,  1050,\n",
       "           1005,  1056,  4527,  2000,  2156,  2027,  2018,  3369,  2431,  2019,\n",
       "           3178,  2077,  1996, 18336,  2318,  1012,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'bert_label': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0, 2007,    0,    0,    0,    0,    0,    0, 1010,    0,    0, 1050,\n",
       "          1005, 1056,    0, 4699,    0,    0,    0,    0,    0, 1998,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0, 1037,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'segment_label': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'is_next': tensor([0, 0])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04f130",
   "metadata": {},
   "source": [
    "#### Visualize encoded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44515720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] usually, he would be tearing around the living room, playing [MASK] his toys. [SEP] after that [MASK] he had digitally [unused689] space been interested in any of the pictures [MASK] emails megan sent. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [CLS] but just one look at [MASK] minion sent him practically catatonic. [SEP] she wasn't surprised to see they had arrived half an hour before the baptism started. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*((batch[\"bert_input\"]))))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e006",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Cant be downloaded automatically from huggingface. Needs to be downloaded manually:\n",
    "\n",
    "1) download from kaggle and \n",
    "2) extract in finetuning folder \n",
    "3) Delete the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2f30c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 159571\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 63978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment\"\n",
    "toxic_dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)\n",
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd0356",
   "metadata": {},
   "source": [
    "#### Test with standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f394a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': ['All Grown Up\\n\\nPlease stop purposefully adding nonsense to Wikipedia.  You have been reported to Wikipedia:Vandalism in progress  19:27, Jun 16, 2005 (UTC)'],\n",
       " 'toxic': tensor([0]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([0]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([0]),\n",
       " 'identity_hate': tensor([0])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88662e6b",
   "metadata": {},
   "source": [
    "#### Standard Tokenizer not sufficient, padding is missing and probably also truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a7997a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2035, 4961, 2039, 3531, 2644, 3800, 7699, 5815, 14652, 2000, 16948, 1012, 2017, 2031, 2042, 2988, 2000, 16948, 1024, 3158, 9305, 2964, 1999, 5082, 2539, 1024, 2676, 1010, 12022, 2385, 1010, 2384, 1006, 11396, 1007, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch[\"comment_text\"])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc5db146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] all grown up please stop purposefully adding nonsense to wikipedia. you have been reported to wikipedia : vandalism in progress 19 : 27, jun 16, 2005 ( utc ) [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*(encoded_input[\"input_ids\"])))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411158c",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65f9e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComment(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows:int=None):\n",
    "        \n",
    "        if not split in [\"train\",\"test\"]:\n",
    "            raise ValueError(\"Parameter has to be 'train' or 'test'\")\n",
    "            \n",
    "        if n_rows is not None:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path, split=split+\"[0:\"+str(n_rows)+\"]\")#[split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)#[split]\n",
    "        \n",
    "        \n",
    "        self.nrows = len(self.dataset) \n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nrows\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        # Step 1: get row\n",
    "        output = self.dataset[item]\n",
    "        #print(output)\n",
    "        \n",
    "        # Step 2: tokenize comment\n",
    "        output[\"bert_input\"] = tokenizer(\n",
    "            output[\"comment_text\"],\n",
    "            max_length=self.seq_len ,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        output.pop(\"comment_text\") #delete raw text\n",
    "        \n",
    "        # Step 3: add bert_label and segment_label like in pretraining task for consistency TODO: Correct?\n",
    "        output[\"bert_label\"] = torch.zeros(self.seq_len)\n",
    "        output[\"segment_label\"] = torch.ones(self.seq_len)\n",
    "        \n",
    "        # Step 4: collect different labels to one tensor \n",
    "        # TODO: desired?\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def get_sent(self, index): #selfmade\n",
    "        '''gets sentence pair as dicitinary s1, s2, isNext'''\n",
    "        isNext = random.random() > 0.5\n",
    "        \n",
    "        t1 = self.dataset[index][\"text\"]\n",
    "        if isNext:\n",
    "            t2 = self.dataset[index+1][\"text\"]\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            t2 = self.get_random_line(index+1)[\"text\"]\n",
    "            return t1, t2, 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a336a4",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8b35e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = ToxicComment(tokenizer=tokenizer, seq_len=SEQ_LEN, split = \"train\", n_rows = 100)\n",
    "len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3c0766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'severe_toxic': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'obscene': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'threat': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'insult': tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'identity_hate': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'bert_input': tensor([[[  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "           18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "            1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "            3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "            1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "            1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "            6486,  1012, 16327,   102]],\n",
       " \n",
       "         [[  101,  1040,  1005, 22091,  2860,   999,  2002,  3503,  2023,  4281,\n",
       "            6120,  1045,  1005,  1049,  9428,  5881,  2007,  1012,  4283,  1012,\n",
       "            1006,  2831,  1007,  2538,  1024,  4868,  1010,  2254,  2340,  1010,\n",
       "            2355,  1006, 11396,  1007,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  4931,  2158,  1010,  1045,  1005,  1049,  2428,  2025,  2667,\n",
       "            2000, 10086,  2162,  1012,  2009,  1005,  1055,  2074,  2008,  2023,\n",
       "            3124,  2003,  7887,  9268,  7882,  2592,  1998,  3331,  2000,  2033,\n",
       "            2083, 10086,  2015,  2612,  1997,  2026,  2831,  3931,  1012,  2002,\n",
       "            3849,  2000,  2729,  2062,  2055,  1996,  4289,  3436,  2084,  1996,\n",
       "            5025, 18558,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1000,  2062,  1045,  2064,  1005,  1056,  2191,  2151,  2613,\n",
       "           15690,  2006,  7620,  1011,  1045,  4999,  2065,  1996,  2930,  6747,\n",
       "            2323,  2022,  2101,  2006,  1010,  2030,  1037,  4942, 29015,  1997,\n",
       "            1000,  1000,  4127,  1997, 13436,  1000,  1000,  1011,  1045,  2228,\n",
       "            1996,  7604,  2089,  2342, 29369,  2075,  2061,  2008,  2027,  2024,\n",
       "            2035,  1999,  1996,  6635,  2168,  4289, 29464,  3058,  4289,  4385,\n",
       "            1012,  1045,  2064,   102]],\n",
       " \n",
       "         [[  101,  2017,  1010,  2909,  1010,  2024,  2026,  5394,  1012,  2151,\n",
       "            3382,  2017,  3342,  2054,  3931,  2008,  1005,  1055,  2006,  1029,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  1000, 23156,  2013,  2033,  2004,  2092,  1010,  2224,  1996,\n",
       "            5906,  2092,  1012,  1087,  2831,  1000,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101, 10338,  6342,  9102,  2077,  2017, 18138,  2105,  2006,  2026,\n",
       "            2147,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  2115,  3158,  9305,  2964,  2000,  1996,  4717, 11895,  2099,\n",
       "           21827,  3720,  2038,  2042, 16407,  1012,  3531,  2123,  1005,  1056,\n",
       "            2079,  2009,  2153,  1010,  2030,  2017,  2097,  2022,  7917,  1012,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]],\n",
       " \n",
       "         [[  101,  3374,  2065,  1996,  2773,  1005, 14652,  1005,  2001,  5805,\n",
       "            2000,  2017,  1012,  4312,  1010,  1045,  1005,  1049,  2025, 16533,\n",
       "            2000,  4339,  2505,  1999,  1996,  3720,  1006, 10166,  2027,  2052,\n",
       "            5376,  2006,  2033,  2005,  3158,  9305,  2964,  1007,  1010,  1045,\n",
       "            1005,  1049,  6414, 17942,  2008,  2009,  2022,  2062,  4372,  5666,\n",
       "           20464, 24174,  2594,  2061,  2028,  2064,  2224,  2009,  2005,  2082,\n",
       "            2004,  1037,  4431,   102]],\n",
       " \n",
       "         [[  101, 12139,  2006,  2023,  3395,  1998,  2029,  2024, 10043,  2000,\n",
       "            2216,  1997,  4241, 15909, 25619,  5004,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0]]]),\n",
       " 'bert_label': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'segment_label': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl2 = DataLoader(test2,batch_size=10,shuffle=False)\n",
    "next(iter(dl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeff4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(dl2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82fd79a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(iter(dl))[\"bert_input\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cf468",
   "metadata": {},
   "source": [
    "## Functions for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b928059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class BertTokenizer():\\n    def __init__(self, task_type=\"pretrain\"):\\n        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\\n            raise ValueError(\"task not implemented\")\\n        pass\\n    \\n    def __call__()'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class BertTokenizer():\n",
    "    def __init__(self, task_type=\"pretrain\"):\n",
    "        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\n",
    "            raise ValueError(\"task not implemented\")\n",
    "        pass\n",
    "    \n",
    "    def __call__()\"\"\"\n",
    "# i noticed we dont need any callable class to do transformation on the datasets since everything is handeled by our dataloaders\n",
    "# ie we dont need rescaling etc.\n",
    "# maybe ask supervisor if we need to save back the tokenized text or if it is okay to do it on the fly and leave the load_data transformation parameter at None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e67880df",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (1178960395.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\morit\\AppData\\Local\\Temp\\ipykernel_2908\\1178960395.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def load_data(dataset:str, transformation=None, n_train:int, n_test:int): # transformation callable\u001b[0m\n\u001b[1;37m                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def load_data(dataset:str, transformation=None, n_train:int, n_test:int=None): # transformation callable\n",
    "    if dataset == \"bookcorpus\":\n",
    "        \n",
    "        \n",
    "        pass\n",
    "    elif dataset == \"jigsaw_toxicity_pred\":\n",
    "        ToxicComment(tokenizer)\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x, outfile:str=None): # can have more args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
