{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97783",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* think about visualizations for text, preprocessing text, etc.\n",
    "* cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "802591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb5848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64 # maximum sequence length\n",
    "VOCAB_SIZE = 30522  # = len(tokenizer.vocab)\n",
    "N_SEGMENTS = 3 # number of segmentation labels\n",
    "EMBED_SIZE = 768 # size of embedding vector\n",
    "DROPOUT = 0.1 # dropout chance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebacda",
   "metadata": {},
   "source": [
    "#### Tokenizer - use pretrained, at least for prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "766ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model_max_length = SEQ_LEN # might not be correct in case of pretraining where we add CLS at the end, check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "012af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose an appropriate tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827bc7f",
   "metadata": {},
   "source": [
    "#### Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cc5fe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncation_side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5adf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # we might need to fixate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2db1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a09c4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a719881",
   "metadata": {},
   "source": [
    "#### Tokenizer example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6afd8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am moritz, who are you ?\"#[\"hi i am moritz\", \"no you are not moritz, you are kevin\"]\n",
    "encoded_input = tokenizer(text)#,padding=True, truncation=True)\n",
    "# , return_tensors='pt') use this for pt tensors\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "432fff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51f874a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi i am moritz, who are you? [SEP]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e006",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Cant be downloaded automatically from huggingface. Needs to be downloaded manually:\n",
    "\n",
    "1) download from kaggle and \n",
    "2) extract in finetuning folder \n",
    "3) Delete the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2f30c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb70fa9f782f459eb6bcd3fa42931139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 159571\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 63978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toxic_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment\"\n",
    "toxic_path = r\"C:\\Users\\Johannes\\Project Machine Learning\\datasets\\finetuning\\toxic_comment\"\n",
    "toxic_dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)\n",
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd0356",
   "metadata": {},
   "source": [
    "#### Test with standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f394a9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comment_text': ['fuck off you stupid aspy asshole'],\n",
       " 'toxic': tensor([1]),\n",
       " 'severe_toxic': tensor([1]),\n",
       " 'obscene': tensor([1]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([1]),\n",
       " 'identity_hate': tensor([0])}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "dataset_length = len(train)\n",
    "print(\"Length of dataset:\", dataset_length)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203ec61",
   "metadata": {},
   "source": [
    "#### Standard Tokenizer not sufficient, padding is missing and probably also truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a7997a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2031, 2017, 2464, 2026, 7514, 2000, 2115, 3437, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch[\"comment_text\"])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc5db146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] have you seen my reply to your answer? [SEP]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*(encoded_input[\"input_ids\"])))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411158c",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "65f9e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComment(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=SEQ_LEN, split=\"train\", n_rows:int=None):\n",
    "        \n",
    "        if not split in [\"train\",\"test\"]:\n",
    "            raise ValueError(\"Parameter has to be 'train' or 'test'\")  \n",
    "        \n",
    "        self.n_rows = n_rows\n",
    "        self.split = split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "            \n",
    "        if self.n_rows is not None:\n",
    "            n_rows_str = f\"[0:{self.n_rows}]\" if self.n_rows is not None else \"\"\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path, split=f\"{self.split}{n_rows_str}\")#[split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)#[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        # Step 1: get row\n",
    "        output = self.dataset[item]\n",
    "\n",
    "        # Step 2: tokenize comment\n",
    "        output[\"input\"] = self.tokenizer(\n",
    "            output[\"comment_text\"],\n",
    "            max_length=self.seq_len ,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # flatten output\n",
    "        output[\"input\"] = output[\"input\"].squeeze()\n",
    "        \n",
    "        output.pop(\"comment_text\") #delete raw text\n",
    "        \n",
    "        # Step 3: add segment_label like in pretraining task for consistency \n",
    "        output[\"segment\"] = torch.ones(self.seq_len)\n",
    "        \n",
    "        # Step 4: collect different labels to one tensor \n",
    "        labels = torch.cat([output[key] if isinstance(output[key], torch.Tensor) else torch.tensor([output[key]]) for key in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],dim=-1)\n",
    "        output[\"labels\"] = labels\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7d0bf",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "12bf7b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ToxicComment' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test2 \u001b[38;5;241m=\u001b[39m ToxicComment(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, seq_len\u001b[38;5;241m=\u001b[39mSEQ_LEN, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(test2)\n",
      "Cell \u001b[1;32mIn[130], line 12\u001b[0m, in \u001b[0;36mToxicComment.__init__\u001b[1;34m(self, tokenizer, seq_len, split, n_rows)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     n_rows_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[0:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjigsaw_toxicity_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_dir\u001b[38;5;241m=\u001b[39mtoxic_path, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mn_rows_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#[split]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjigsaw_toxicity_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_dir\u001b[38;5;241m=\u001b[39mtoxic_path)\u001b[38;5;66;03m#[split]\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ToxicComment' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "test2 = ToxicComment(tokenizer=tokenizer, seq_len=SEQ_LEN, split = \"train\", n_rows = 100)\n",
    "len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3c0766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': tensor([0]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([0]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([0]),\n",
       " 'identity_hate': tensor([0]),\n",
       " 'input': tensor([[  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "          18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "           1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "           3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "           1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "           1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "           6486,  1012, 16327,   102]]),\n",
       " 'segment': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " 'labels': tensor([[0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl2 = DataLoader(test2,batch_size=1,shuffle=False)\n",
    "batch = next(iter(dl2))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eeff4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82fd79a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"input\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed0852",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fc6b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, embed_size, seq_len):\n",
    "        super().__init__()\n",
    "        n = 10000 # scalar for pos encoding\n",
    "        # create embedding matrix dim(seq_len  x embed_size)\n",
    "        self.embed_matrix = torch.zeros(seq_len, embed_size).float()\n",
    "        # positional encoding not to be updated while gradient descent\n",
    "        self.embed_matrix.require_grad = False\n",
    "        \n",
    "        # compute embedding for each position in input\n",
    "        for position in range(seq_len):\n",
    "            # run trough every component of embedding vector for each position with stride 2\n",
    "            for c in range(0, embed_size, 2):\n",
    "                # even \n",
    "                self.embed_matrix[position,c] = math.sin(position/(n**(2*c/embed_size)))\n",
    "                # uneven\n",
    "                self.embed_matrix[position,c+1] = math.cos(position/(n**(2*c/embed_size)))\n",
    "        \n",
    "        # self.embed_matrix =  embed_matrix.unsqueeze(0) \n",
    "    def forward(self, x):\n",
    "        return self.embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "946b07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embed_size=EMBED_SIZE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        # token embedding: transforms (vocabulary size, number of tokens) into (vocabulary size, number of tokens, length of embdding vector)\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0) # padding remains 0 during training\n",
    "        # embedding of position\n",
    "        self.position = PositionEmbedding(embed_size, seq_len) \n",
    "        # droput probability per token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        return self.dropout(self.token(sequence) + self.position(sequence))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "42c4a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_seq size torch.Size([64])\n",
      "tensor([  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
      "        18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
      "         1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
      "         3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
      "         1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
      "         1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
      "         6486,  1012, 16327,   102])\n",
      "torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "# embedding test: tokenized sequence\n",
    "sample_seq = batch['input'][0] \n",
    "print(f'sample_seq size {sample_seq.size()}')\n",
    "print(sample_seq)\n",
    "\n",
    "bert = BERTEmbedding(VOCAB_SIZE, SEQ_LEN)\n",
    "\n",
    "batch_embed = bert(batch['input'][0].long())\n",
    "\n",
    "print(batch_embed.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107853a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "759b5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention heads\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, number_heads, model_dimension):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # model dimension must be divideable into equal parts for the attention heads\n",
    "        assert model_dimension%number_heads == 0\n",
    "        self.number_heads = number_heads\n",
    "        self.att_head_dim = int(model_dimension/number_heads)\n",
    "        \n",
    "        # attention mechanism: query, key, value are linear embeddings -> embedding matrix dim: (model_dimension x model_dimension)\n",
    "        self.query = nn.Linear(model_dimension, model_dimension)\n",
    "        self.key = nn.Linear(model_dimension, model_dimension)\n",
    "        self.value = nn.Linear(model_dimension, model_dimension)\n",
    "        self.lin_output = nn.Linear(model_dimension, model_dimension)\n",
    "    \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \n",
    "        # output dim (batch_size x seq_len x model_dimension) \n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value) \n",
    "        \n",
    "        # transform q,k,v to fit attention heads:(batch_size x seq_len x model_dimension) -> (batch_size x number_heads x seq_len x att_head_dim)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.number_heads, self.att_head_dim)\n",
    "        query = query.permute(0,2,1,3)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.number_heads, self.att_head_dim)\n",
    "        key = key.permute(0,2,1,3)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.number_heads, self.att_head_dim)\n",
    "        value = value.permute(0,2,1,3)\n",
    "        \n",
    "        # calculate dot product between each query and each key and normaliz the output, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score = torch.matmul(query, key.permute(0, 1, 3, 2)) \n",
    "        score_n = score / math.sqrt(self.att_head_dim) # normalize: <q,k>/sqrt(d_k)\n",
    "        \n",
    "        # mask 0 with -infinity so it becomes 0 after softmax, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score_m = score_n.masked_fill(mask == 0, -1e10)    \n",
    "        \n",
    "        # softmax scores along each query, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score_w = nn.functional.softmax(score_m, dim=-1) \n",
    "        \n",
    "        # multiply with value matrix: output weighted sum for each query, output dim: (batch_size x number_heads x seq_len x att_head_dim)\n",
    "        weighted_sum = torch.matmul(score_w, value)\n",
    "        \n",
    "        # concatenate attention heads to 1 output, output dim: (batch_size x number_heads x model_dimension)\n",
    "        weighted_sum = weighted_sum.permute(0, 2, 1, 3).contiguous().view(weighted_sum.shape[0], -1, self.number_heads * self.att_head_dim)\n",
    "        \n",
    "        # linear embedding for output\n",
    "        out = self.lin_output(weighted_sum)      \n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c977081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, hidden_dimension):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        \n",
    "        # linear layer\n",
    "        self.linear1 = nn.Linear(model_dimension, hidden_dimension)\n",
    "        self.linear2 = nn.Linear(hidden_dimension, model_dimension)\n",
    "        # non-linearity\n",
    "        self.non_linear = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.non_linear(self.linear1(x)))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "870b484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder stacks together all the previous modules\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_dimension=EMBED_SIZE, number_heads=12, ff_hidden_dim=EMBED_SIZE*4):\n",
    "        super(Encoder, self).__init__()\n",
    "        # attention heads\n",
    "        self.multihead_attention = MultiHeadAttention (number_heads, model_dimension)\n",
    "        # normalisation layer\n",
    "        self.normlayer = nn.LayerNorm(model_dimension)\n",
    "        self.feedforward_layer = FeedForwardLayer(model_dimension, hidden_dimension=ff_hidden_dim)\n",
    "    \n",
    "    # also residuals possible here\n",
    "    def forward(self, x, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        # input x 3x to generate query, key, value\n",
    "        x = self.normlayer(self.multihead_attention(x, x, x, mask))\n",
    "        return self.normlayer(self.feedforward_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82f4474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class for BERT\n",
    "class BERTBase(nn.Module):\n",
    "    # __init__ function takes hyperparameters, initializes the model accordingly and sets up trainable parameters\n",
    "    def __init__(self, vocab_size, model_dimension, number_layers, number_heads):\n",
    "        super().__init__()\n",
    "        self.model_dimension=model_dimension\n",
    "        self.number_layers=number_layers\n",
    "        self.number_heads=number_heads\n",
    "        # hidden layer dimenion of FF is 4*model_dimension (see paper)\n",
    "        self.ff_hidden_layer = 4*model_dimension\n",
    "        # embedding of input \n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, seq_len=SEQ_LEN, embed_size=model_dimension)\n",
    "        # stack encoders\n",
    "        self.encoders = torch.nn.ModuleList() # create empty module list\n",
    "        for _ in range(self.number_layers):\n",
    "            self.encoders.append(Encoder(model_dimension=model_dimension, number_heads=number_heads, ff_hidden_dim=4*model_dimension))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # mask to mark the padded tokens\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1,x.size(1),1).unsqueeze(1)\n",
    "        x = self.embedding(x) \n",
    "        # run trough encoders\n",
    "        for encoder in self.encoders:\n",
    "            x =encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33c67f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning\n",
    "class ToxicityPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    class to predict multivariate class of toxicity\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_out):\n",
    "        super().__init__()\n",
    "        self.tox_classes = 6 # there are 6 classes of toxicity in the dataset\n",
    "        self.linear = nn.Linear(bert_out, self.tox_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1) # not necessary, included in torch.nn.CrossEntropyLoss\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # recieve output dimension (batch_size, self.tox_classes)\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66999e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class according to Milestone 1 task sheet\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_dimension, number_layers=12, number_heads=12):\n",
    "        super().__init__()\n",
    "        # base BERT model\n",
    "        self.base_model = BERTBase(vocab_size, model_dimension, number_layers, number_heads)\n",
    "        # toxic comment classfication layer\n",
    "        self.toxic_comment = ToxicityPrediction(self.base_model.model_dimension)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return self.toxic_comment(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0c699",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "afb04877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainBERT:\n",
    "    def __init__(self, model, train_dataloader, epochs, test_dataloader=None, learning_rate=0.001, threshold=0.5, device='cuda'):\n",
    "        \n",
    "        # hyperparameters for optimization\n",
    "        self.device = device\n",
    "        self.bar = None\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.training_data = train_dataloader\n",
    "        self.testing_data = test_dataloader\n",
    "\n",
    "        # optimizer: Adam\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # learning rate scheduler\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # cost function cross entropy loss for predicting classes of toxicity\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # predictions threshold above which predictions are set True\n",
    "        self.threshold = threshold \n",
    "        \n",
    "        # run training\n",
    "        for epoch in range(self.epochs):\n",
    "            self.training(epoch)\n",
    "\n",
    "    def training(self, epoch):\n",
    "        # init stats\n",
    "        avg_loss = 0.0\n",
    "        corrects_sum = 0\n",
    "        trues_sum = 0\n",
    "        \n",
    "        # set back progress bar\n",
    "        self.bar = None\n",
    "        # create new progress bar\n",
    "        self.bar = tqdm(total=len(self.training_data.dataset), desc=f'Training epoch {epoch+1}', leave=True, position=0)\n",
    "\n",
    "        for i, data in enumerate(self.training_data):\n",
    "            \n",
    "            # send data to GPU/CPU\n",
    "            data ={key: value.to(self.device) for key, value in data.items()}\n",
    "            \n",
    "            # labels convert to float()\n",
    "            labels = data['labels'].float()\n",
    "            \n",
    "            # forward pass: comments trough model\n",
    "            output = self.model.forward(data['input'])\n",
    "            \n",
    "            # compute loss with labels (input, target)\n",
    "            loss = self.criterion(output, labels)\n",
    "            \n",
    "            # backward pass for training\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # average loss per batch\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            # compute accuracy \n",
    "            # softmax the output vector to get probabilites\n",
    "            predictions = nn.functional.softmax(output, dim=1)\n",
    "            # use threshold to determine which of the outputs are considered True\n",
    "            predictions = torch.ge(predictions, self.threshold).int()\n",
    "            # compare with the label and count correct classifications\n",
    "            corrects_sum += (predictions == labels).sum().item()\n",
    "            # sump up total number of Trues in labels for batch\n",
    "            trues_sum += labels.nelement()\n",
    "            \n",
    "            # update progress bar\n",
    "            self.bar.update(self.training_data.batch_size)\n",
    "        \n",
    "        # update learning rate scheduler\n",
    "        self.scheduler.step() \n",
    "        # print stats\n",
    "        print(f'Trainig epoch: {epoch+1}\\nAvg. training loss: {(avg_loss / len(self.training_data)):.2f}\\nAccuracy: {(corrects_sum * 100.0 / trues_sum):.2f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e2a6901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1:   0%|                                                                        | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m bert_trainer \u001b[38;5;241m=\u001b[39m TrainBERT(bert_lm, train_loader, epochs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[118], line 26\u001b[0m, in \u001b[0;36mTrainBERT.__init__\u001b[1;34m(self, model, train_dataloader, epochs, test_dataloader, learning_rate, threshold, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining(epoch)\n",
      "Cell \u001b[1;32mIn[118], line 55\u001b[0m, in \u001b[0;36mTrainBERT.training\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# backward pass for training\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# average loss per batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training test\n",
    "# set up tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# load test dataset\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=100, n_test=None)\n",
    "\n",
    "# set up dataloader\n",
    "train_loader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "\n",
    "# set up BERT model\n",
    "bert = Model(vocab_size=VOCAB_SIZE, model_dimension=EMBED_SIZE, number_layers=12, number_heads=12)\n",
    "\n",
    "# number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# train model\n",
    "bert_trainer = TrainBERT(bert_lm, train_loader, epochs, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dd3cd5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n",
      "Training epoch 1:   0%|                                                                      | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m bert_trainer \u001b[38;5;241m=\u001b[39m TrainBERT(bert_lm, train_loader, epochs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[124], line 25\u001b[0m, in \u001b[0;36mTrainBERT.__init__\u001b[1;34m(self, model, train_dataloader, epochs, test_dataloader, learning_rate, threshold, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# run training\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining(epoch)\n",
      "Cell \u001b[1;32mIn[124], line 54\u001b[0m, in \u001b[0;36mTrainBERT.training\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# backward pass for training\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# average loss per batch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training (for cluster)\n",
    "# set up tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# load the entire training data (length 10000) into train\n",
    "train, _ = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=10000, n_test=None)\n",
    "\n",
    "# set up dataloader\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
    "\n",
    "# set up BERT model\n",
    "bert_lm = Model(vocab_size=VOCAB_SIZE, model_dimension=EMBED_SIZE, number_layers=12, number_heads=12)\n",
    "\n",
    "# number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# train model (device to be updated according to cluster GPU)\n",
    "bert_trainer = TrainBERT(bert_lm, train_loader, epochs, device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada6b40",
   "metadata": {},
   "source": [
    "## Functions for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class BertTokenizer():\n",
    "    def __init__(self, task_type=\"pretrain\"):\n",
    "        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\n",
    "            raise ValueError(\"task not implemented\")\n",
    "        pass\n",
    "    \n",
    "    def __call__()\"\"\"\n",
    "# i noticed we dont need any callable class to do transformation on the datasets since everything is handeled by our dataloaders\n",
    "# ie we dont need rescaling etc.\n",
    "# maybe ask supervisor if we need to save back the tokenized text or if it is okay to do it on the fly and leave the load_data transformation parameter at None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7bf4a2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "def load_data(dataset:str, transformation=None, n_train:int=None, n_test:int=None): # transformation callable\n",
    "    \n",
    "    if dataset == \"bookcorpus\":\n",
    "        train = Bookcorpus(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        return train, None\n",
    "    \n",
    "    elif dataset == \"jigsaw_toxicity_pred\":\n",
    "        train = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        \n",
    "        test = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"test\",\n",
    "            n_rows=n_test\n",
    "        )\n",
    "        return train, test\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "750c0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a85a9ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'input': tensor([  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "         18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "          1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "          3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "          1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "          1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "          6486,  1012, 16327,   102]),\n",
       " 'segment': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c46fb18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'toxic': 0,\n",
       " 'severe_toxic': 0,\n",
       " 'obscene': 0,\n",
       " 'threat': 0,\n",
       " 'insult': 0,\n",
       " 'identity_hate': 0,\n",
       " 'input': tensor([ 101, 4067, 2017, 2005, 4824, 1012, 1045, 2228, 2200, 3811, 1997, 2017,\n",
       "         1998, 2052, 2025, 7065, 8743, 2302, 6594, 1012,  102,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'segment': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ac722bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"bookcorpus\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2762923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7c1552ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2645215428.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[209], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def show(x, outfile:str=None): # can have more args\u001b[0m\n\u001b[1;37m                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def show(x, outfile:str=None): # can have more args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
