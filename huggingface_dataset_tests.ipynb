{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97783",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Ensure that during pre training, both sentences fit into the model at the same time -> DONE but not tested\n",
    "* think about visualizations for text, preprocessing text, etc.\n",
    "* implement interfaces of the task sheet\n",
    "* cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "802591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "cb5848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64 # maximum sequence length\n",
    "VOCAB_SIZE = 30522  # = len(tokenizer.vocab)\n",
    "N_SEGMENTS = 3 # number of segmentation labels\n",
    "EMBED_SIZE = 768 # size of embedding vector\n",
    "DROPOUT = 0.1 # dropout chance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464fe5e",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc737c1",
   "metadata": {},
   "source": [
    "### bookcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "5b594b1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (C:/Users/Johannes/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df426e6c882f403d8765bd8818e6f10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 74004228\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Download + load data from cache or online AUTOMATICALLY\n",
    "# https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bookcorpus\") # alternative, less size datasets.load_dataset(\"bookcorpus\", split=\"train[:10%]\")\n",
    "# split=\"train[10:20]\")\n",
    "# saved here on windows C:\\Users\\morit\\.cache\\huggingface\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f56493",
   "metadata": {},
   "source": [
    "#### Saving huggingface Dataset on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "30a6ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual save to disk\n",
    "\n",
    "#folder_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\pretraining\"\n",
    "#full_path = folder_path+r\"\\bookcorpus\"\n",
    "\n",
    "#dataset.save_to_disk(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cfd29",
   "metadata": {},
   "source": [
    "#### Loading hf dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "6a77b037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manual load from disk\n",
    "\n",
    "#dataset = datasets.load_dataset(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4604f83",
   "metadata": {},
   "source": [
    "#### slicing hf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "c14821a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her parents rattled along to each other as they made their way through the tree-lined suburbs where megan had grown up .'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][66][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c4a55",
   "metadata": {},
   "source": [
    "#### Standard dataloader - not sufficient we need tokenized output -> implement own dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "15cb2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['usually , he would be tearing around the living room , playing with his toys .',\n",
       "  'but just one look at a minion sent him practically catatonic .']}"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset[\"train\"], batch_size=2)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebacda",
   "metadata": {},
   "source": [
    "#### Tokenizer - use pretrained, at least for prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "012af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose an appropriate tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "766ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model_max_length = SEQ_LEN # might not be correct in case of pretraining where we add CLS at the end, check that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827bc7f",
   "metadata": {},
   "source": [
    "#### Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "7cc5fe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncation_side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "d5adf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # we might need to fixate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "b2db1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a09c4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a719881",
   "metadata": {},
   "source": [
    "#### Tokenizer example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "6afd8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am moritz, who are you ?\"#[\"hi i am moritz\", \"no you are not moritz, you are kevin\"]\n",
    "encoded_input = tokenizer(text)#,padding=True, truncation=True)\n",
    "# , return_tensors='pt') use this for pt tensors\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "432fff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "51f874a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi i am moritz, who are you? [SEP]'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b4d7e",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "20979eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = None\n",
    "n_rows is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f8564c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "class Bookcorpus(Dataset):  \n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "        \"\"\"\n",
    "        n_rows == None means take the whole dataset\n",
    "        \"\"\"\n",
    "     \n",
    "        if not split in [\"train\"]:\n",
    "            raise ValueError(\"For Bookcorpus there is only a train split\")\n",
    "            \n",
    "        self.n_rows = n_rows # is only inititialized if __len__() is called\n",
    "        self.tokenizer = tokenizer \n",
    "        self.seq_len = seq_len\n",
    "        self.split = split\n",
    "        self.dataset = None # only loaded id needed\n",
    "    \n",
    "    # apply lazy loading\n",
    "    def load_memory(self):\n",
    "        if self.n_rows is not None:\n",
    "            self.dataset = load_dataset(\"bookcorpus\", split=self.split+\"[0:\"+str(self.n_rows)+\"]\") # [split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"bookcorpus\") # [split]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset is None:\n",
    "            self.load_memory() # only loaded if required\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item): \n",
    "        if self.dataset is None:\n",
    "            self.load_memory() # only loaded if required\n",
    "        \n",
    "        # Create a random pair of sentences, if is_next is true if they are subsequent\n",
    "        s1, s2, is_next = self.get_sentence_pair(item)\n",
    "        \n",
    "        # Replace 15% of the words in each line with masks/random words/the word itself\n",
    "        s1_random, s1_label = self.random_masking(s1)\n",
    "        s2_random, s2_label = self.random_masking(s2)\n",
    "        \n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences # copied \n",
    "         # Adding PAD token for labels\n",
    "        cls = [self.tokenizer.vocab['[CLS]']]\n",
    "        sep = [self.tokenizer.vocab['[SEP]']] \n",
    "        pad = [self.tokenizer.vocab['[PAD]']]\n",
    "        \n",
    "        # append separating tokens to sequence       \n",
    "        s1 = cls + s1_random + sep       \n",
    "        s2 = s2_random + sep\n",
    "        s1_label = pad + s1_label + pad\n",
    "        s2_label = s2_label + pad\n",
    "               \n",
    "        # add segement label, adding padding\n",
    "        segment = ([1 for i in range(len(s1))]+[2 for i in range(len(s2))])[:self.seq_len]\n",
    "        # generate 1 input for model\n",
    "        model_input = (s1+s2)[:self.seq_len]\n",
    "        model_label = (s1_label + s2_label)[:self.seq_len]\n",
    "        # add padding where input is shorter than sequence\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(model_input))]\n",
    "        model_input.extend(padding)\n",
    "        model_label.extend(padding)\n",
    "        segment.extend(padding)\n",
    "        \n",
    "\n",
    "\n",
    "        output = {\n",
    "            \"input\": torch.tensor(model_input),\n",
    "            \"label\": torch.tensor(model_label),\n",
    "            \"segment\": torch.tensor(segment),\n",
    "            \"is_next\": torch.tensor(is_next)\n",
    "        }\n",
    "\n",
    "        return {key: value.clone().detach() for key, value in output.items()}        \n",
    "        #return  {\"s1\":s1, \"s2\":s2, \"is_next_label\":is_next_label}\n",
    "        #return {\"t1_random\":t1_random, \"t1_label\":t1_label, \"t2_random\":t2_random, \"t2_label\":t2_label}\n",
    "    \n",
    "    def get_sentence_pair(self, index): \n",
    "        '''gets sentence pair as dicitinary s1, s2, isNext'''\n",
    "        isNext = random.random() > 0.5 # if number > 0.5 isNext is positive\n",
    "        \n",
    "        s1 = self.dataset[index][\"text\"]\n",
    "        if isNext and index + 1 < len(self.dataset): # select two subsequent lines\n",
    "            s2 = self.dataset[index+1][\"text\"]\n",
    "            return s1, s2, 1 # line1, line2, subsequent\n",
    "        else: # select two non-Subsequent lines (index+1 is excluded from random selection)\n",
    "            s2 = self.get_random_line(index+1)[\"text\"]\n",
    "            return s1, s2, 0 # line1, line2, subsequent\n",
    "        \n",
    "    def get_random_line(self, excludedIndex): \n",
    "        '''return random single sentence excluding'''\n",
    "        randIndex = random.randint(1, self.__len__()-1)\n",
    "            \n",
    "        # ensure that randIndex is not next sentence\n",
    "        while randIndex == excludedIndex:\n",
    "            randIndex = random.randint(1, self.__len__()-1)\n",
    "        \n",
    "        return self.dataset[randIndex]\n",
    "    \n",
    "    def random_masking(self, sentence):\n",
    "        words = sentence.split()\n",
    "        masked_out = []\n",
    "        masked_labels = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            rnd_number1 = random.random() # continuous number from [0,1]\n",
    "            rnd_number2 = random.random() # continuous number from [0,1]\n",
    "\n",
    "            # turn word into token, remove [CLS], [SEP]\n",
    "            token = self.tokenizer(word)['input_ids'] \n",
    "            token = token[1:-1]\n",
    "\n",
    "            # replace a word with a probability of 15%\n",
    "            if rnd_number1 < 0.15:\n",
    "\n",
    "                # with 80% chance replace word by mask\n",
    "                if rnd_number2 < 0.8:\n",
    "                    for j in range(len(token)):\n",
    "                        masked_out.append(self.tokenizer.vocab['[MASK]'])\n",
    "                # with 10% chance replace word by random word\n",
    "                elif rnd_number2 < 0.9:\n",
    "                    for k in range(len(token)):\n",
    "                        masked_out.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "                # with 10% chance word remains\n",
    "                else:\n",
    "                    masked_out.append(token)\n",
    "\n",
    "                # set corresponding label\n",
    "                masked_labels.append(token)\n",
    "            # 85% don't change anything\n",
    "            else:\n",
    "                masked_out.append(token)\n",
    "                # create corrsponding 0-label\n",
    "                for l in range(len(token)):\n",
    "                    masked_labels.append(0)\n",
    "                \n",
    "        # flatten output\n",
    "        masked_out = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in masked_out]))\n",
    "        print(masked_out)\n",
    "        masked_labels = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in masked_labels]))\n",
    "        print(masked_labels)\n",
    "\n",
    "        # check for correct length\n",
    "        assert len(masked_out) == len(masked_labels)\n",
    "        #assert len(output) == self.seq_len, \"sequence length not fixed! \"+str(len(output)) # from moritz\n",
    "        return masked_out, masked_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e21d7",
   "metadata": {},
   "source": [
    "#### Testing the Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "40334c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (C:/Users/Johannes/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Bookcorpus(tokenizer, n_rows = 100)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "8b123188",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(test,batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "9ed0ac1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(1,1000):\\n    batch = next(iter(dl))\\n    for j in range(1,2): # batchsize\\n        length_ = len(batch[\"bert_input\"][j])\\n        #print(length_)\\n        assert length_==SEQ_LEN, \"sequence size is not \"+str(SEQ_LEN)+\": \"+ str(length_)'"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#is sequence length fixed?\n",
    "\"\"\"for i in range(1,1000):\n",
    "    batch = next(iter(dl))\n",
    "    for j in range(1,2): # batchsize\n",
    "        length_ = len(batch[\"bert_input\"][j])\n",
    "        #print(length_)\n",
    "        assert length_==SEQ_LEN, \"sequence size is not \"+str(SEQ_LEN)+\": \"+ str(length_)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "feee7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2788, 1010, 2002, 2052, 2022, 13311, 103, 1996, 2542, 2282, 1010, 2652, 2007, 2010, 10899, 1012]\n",
      "[0, 0, 0, 0, 2022, 0, 2105, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2021, 2074, 2028, 2298, 2012, 1037, 7163, 2239, 2741, 2032, 8134, 4937, 22436, 2594, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2021, 2074, 2028, 2298, 2012, 1037, 7163, 2239, 2741, 2032, 103, 4937, 22436, 2594, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8134, 0, 0, 0, 0]\n",
      "[2008, 2018, 2042, 12756, 1005, 1055, 2933, 2043, 2016, 103, 2032, 5102, 3041, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 2288, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[  101,  2788,  1010,  2002,  2052,  2022, 13311,   103,  1996,  2542,\n",
       "           2282,  1010,  2652,  2007,  2010, 10899,  1012,   102,  2021,  2074,\n",
       "           2028,  2298,  2012,  1037,  7163,  2239,  2741,  2032,  8134,  4937,\n",
       "          22436,  2594,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0],\n",
       "         [  101,  2021,  2074,  2028,  2298,  2012,  1037,  7163,  2239,  2741,\n",
       "           2032,   103,  4937, 22436,  2594,  1012,   102,  2008,  2018,  2042,\n",
       "          12756,  1005,  1055,  2933,  2043,  2016,   103,  2032,  5102,  3041,\n",
       "           1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0]]),\n",
       " 'label': tensor([[   0,    0,    0,    0,    0, 2022,    0, 2105,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 8134,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0, 2288,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'segment': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'is_next': tensor([1, 1])}"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6b813",
   "metadata": {},
   "source": [
    "#### Visualize encoded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "44515720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] usually, he would be tearing [MASK] the living room, playing with his toys. [SEP] but just one look at a minion sent him practically catatonic. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [CLS] but just one look at a minion sent him [MASK] catatonic. [SEP] that had been megan's plan when she [MASK] him dressed earlier. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*((batch[\"input\"]))))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828f7d6",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "4fc6b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, embed_size, seq_len):\n",
    "        super().__init__()\n",
    "        n = 10000 # scalar for pos encoding\n",
    "        # create embedding matrix dim(seq_len  x embed_size)\n",
    "        self.embed_matrix = torch.zeros(seq_len, embed_size).float()\n",
    "        # positional encoding not to be updated while gradient descent\n",
    "        self.embed_matrix.require_grad = False\n",
    "        \n",
    "        # compute embedding for each position in input\n",
    "        for position in range(seq_len):\n",
    "            # run trough every component of embedding vector for each position with stride 2\n",
    "            for c in range(0, embed_size, 2):\n",
    "                # even \n",
    "                self.embed_matrix[position,c] = math.sin(position/(n**(2*c/embed_size)))\n",
    "                # uneven\n",
    "                self.embed_matrix[position,c+1] = math.cos(position/(n**(2*c/embed_size)))\n",
    "        \n",
    "        # self.embed_matrix =  embed_matrix.unsqueeze(0) \n",
    "    def forward(self, x):\n",
    "        return self.embed_matrix\n",
    "            \n",
    "\n",
    "class BERTEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, n_segments=N_SEGMENTS, embed_size=EMBED_SIZE, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        # token embedding: transforms (vocabulary size, number of tokens) into (vocabulary size, number of tokens, length of embdding vector)\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0) # padding remains 0 during training\n",
    "        # segment embedding for sentence 1, sentence 2, padding\n",
    "        self.segment = nn.Embedding(n_segments, embed_size, padding_idx=0)\n",
    "        # embedding of position\n",
    "        self.position = PositionEmbedding(embed_size, seq_len) \n",
    "        # droput probability per token\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, sequence, seg_label):\n",
    "        return self.dropout(self.token(sequence) + self.segment(seg_label) + self.position(sequence))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "75a36f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "tensor([  101,  2788,  1010,  2002,  2052,  2022, 13311,   103,  1996,  2542,\n",
      "         2282,  1010,  2652,  2007,  2010, 10899,  1012,   102,  2021,  2074,\n",
      "         2028,  2298,  2012,  1037,  7163,  2239,  2741,  2032,  8134,  4937,\n",
      "        22436,  2594,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "torch.Size([64])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "# embedding test: tokenized sequence\n",
    "sample_seq = batch['input'][0] \n",
    "sample_seg = batch['segment'][0]\n",
    "print(sample_seq.size())\n",
    "print(sample_seq)\n",
    "print(sample_seg.size())\n",
    "print(sample_seg)\n",
    "\n",
    "bert = BERTEmbedding(VOCAB_SIZE, SEQ_LEN, N_SEGMENTS)\n",
    "\n",
    "batch_embed = bert(batch['input'][0], batch['segment'][0])\n",
    "\n",
    "print(batch_embed.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e006",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Cant be downloaded automatically from huggingface. Needs to be downloaded manually:\n",
    "\n",
    "1) download from kaggle and \n",
    "2) extract in finetuning folder \n",
    "3) Delete the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b2f30c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee66e17b391e49c7adc005a12b1859d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 159571\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 63978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toxic_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment\"\n",
    "toxic_path = r\"C:\\Users\\Johannes\\Project Machine Learning\\datasets\\finetuning\\toxic_comment\"\n",
    "toxic_dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)\n",
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd0356",
   "metadata": {},
   "source": [
    "#### Test with standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f394a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_text': ['Hi enemy\\n\\nYou will NEVER be able to get rid of me faggot.'],\n",
       " 'toxic': tensor([1]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([1]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([1]),\n",
       " 'identity_hate': tensor([1])}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203ec61",
   "metadata": {},
   "source": [
    "#### Standard Tokenizer not sufficient, padding is missing and probably also truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "2a7997a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 4099, 2017, 2097, 2196, 2022, 2583, 2000, 2131, 9436, 1997, 2033, 6904, 13871, 4140, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch[\"comment_text\"])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "dc5db146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi enemy you will never be able to get rid of me faggot. [SEP]'"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "flattened = list(chain(*(encoded_input[\"input_ids\"])))\n",
    "tokenizer.decode(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411158c",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "65f9e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComment(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, seq_len=SEQ_LEN, split=\"train\", n_rows:int=None):\n",
    "        \n",
    "        if not split in [\"train\",\"test\"]:\n",
    "            raise ValueError(\"Parameter has to be 'train' or 'test'\")       \n",
    "        \n",
    "        self.dataset = None # only loaded if needed\n",
    "        self.n_rows = n_rows # only loaded if needed\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.split = split\n",
    "        \n",
    "        \n",
    "    # apply lazy loading\n",
    "    def load_memory(self):\n",
    "        if self.n_rows is not None:\n",
    "            n_rows_str = f\"[0:{self.n_rows}]\" if self.n_rows is not None else \"\"\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path, split=f\"{self.split}{n_rows_str}\")#[split]\n",
    "        else:\n",
    "            self.dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)#[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset is None:\n",
    "            self.load_memory() # only loaded if required\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.dataset is None:\n",
    "            self.load_memory() # only loaded if required\n",
    "        \n",
    "        # Step 1: get row\n",
    "        output = self.dataset[item]\n",
    "\n",
    "        # Step 2: tokenize comment\n",
    "        output[\"input\"] = self.tokenizer(\n",
    "            output[\"comment_text\"],\n",
    "            max_length=self.seq_len ,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        # flatten output\n",
    "        output[\"input\"] = output[\"input\"].squeeze()\n",
    "        \n",
    "        output.pop(\"comment_text\") #delete raw text\n",
    "        \n",
    "        # Step 3: add segment_label like in pretraining task for consistency \n",
    "        # output[\"label\"] = torch.zeros(self.seq_len) # this is not embedded anywhere so we can cut this\n",
    "        output[\"segment\"] = torch.ones(self.seq_len)\n",
    "        \n",
    "        # Step 4: collect different labels to one tensor \n",
    "        # TODO: desired?\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def get_sent(self, index): #selfmade\n",
    "        '''gets sentence pair as dicitinary s1, s2, isNext'''\n",
    "        isNext = random.random() > 0.5\n",
    "        \n",
    "        t1 = self.dataset[index][\"text\"]\n",
    "        if isNext:\n",
    "            t2 = self.dataset[index+1][\"text\"]\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            t2 = self.get_random_line(index+1)[\"text\"]\n",
    "            return t1, t2, 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7d0bf",
   "metadata": {},
   "source": [
    "#### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "12bf7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = ToxicComment(tokenizer=tokenizer, seq_len=SEQ_LEN, split = \"train\", n_rows = 100)\n",
    "len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "d3c0766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': tensor([0]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([0]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([0]),\n",
       " 'identity_hate': tensor([0]),\n",
       " 'input': tensor([[  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
       "          18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
       "           1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
       "           3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
       "           1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
       "           1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
       "           6486,  1012, 16327,   102]]),\n",
       " 'segment': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl2 = DataLoader(test2,batch_size=1,shuffle=False)\n",
    "batch = next(iter(dl2))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "eeff4286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "82fd79a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"input\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed0852",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "42c4a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_seq size torch.Size([64])\n",
      "tensor([  101,  7526,  2339,  1996, 10086,  2015,  2081,  2104,  2026,  5310,\n",
      "        18442, 13076, 12392,  2050,  5470,  2020, 16407,  1029,  2027,  4694,\n",
      "         1005,  1056,  3158,  9305, 22556,  1010,  2074,  8503,  2006,  2070,\n",
      "         3806,  2044,  1045,  5444,  2012,  2047,  2259, 14421,  6904,  2278,\n",
      "         1012,  1998,  3531,  2123,  1005,  1056,  6366,  1996, 23561,  2013,\n",
      "         1996,  2831,  3931,  2144,  1045,  1005,  1049,  3394,  2085,  1012,\n",
      "         6486,  1012, 16327,   102])\n",
      "sample_seg size torch.Size([64])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "# embedding test: tokenized sequence\n",
    "sample_seq = batch['input'][0] \n",
    "sample_seg = batch['segment'][0]\n",
    "print(f'sample_seq size {sample_seq.size()}')\n",
    "print(sample_seq)\n",
    "print(f'sample_seg size {sample_seg.size()}')\n",
    "print(sample_seg)\n",
    "\n",
    "bert = BERTEmbedding(VOCAB_SIZE, SEQ_LEN)\n",
    "\n",
    "batch_embed = bert(batch['input'][0].long(), batch['segment'][0].long())\n",
    "\n",
    "print(batch_embed.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107853a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "759b5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# attention heads\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, number_heads, model_dimension):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # model dimension must be divideable into equal parts for the attention heads\n",
    "        assert model_dimension%number_heads == 0\n",
    "        self.number_heads = number_heads\n",
    "        self.att_head_dim = int(model_dimension/number_heads)\n",
    "        \n",
    "        # attention mechanism: query, key, value are linear embeddings -> embedding matrix dim: (model_dimension x model_dimension)\n",
    "        self.query = nn.Linear(model_dimension, model_dimension)\n",
    "        self.key = nn.Linear(model_dimension, model_dimension)\n",
    "        self.value = nn.Linear(model_dimension, model_dimension)\n",
    "        self.lin_output = nn.Linear(model_dimension, model_dimension)\n",
    "    \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \n",
    "        # output dim (batch_size x seq_len x model_dimension) \n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value) \n",
    "        \n",
    "        # transform q,k,v to fit attention heads:(batch_size x seq_len x model_dimension) -> (batch_size x number_heads x seq_len x att_head_dim)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.number_heads, self.att_head_dim)\n",
    "        query = query.permute(0,2,1,3)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.number_heads, self.att_head_dim)\n",
    "        key = key.permute(0,2,1,3)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.number_heads, self.att_head_dim)\n",
    "        value = value.permute(0,2,1,3)\n",
    "        \n",
    "        # calculate dot product between each query and each key and normaliz the output, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score = torch.matmul(query, key.permute(0, 1, 3, 2)) \n",
    "        score_n = score / math.sqrt(self.att_head_dim) # normalize: <q,k>/sqrt(d_k)\n",
    "        \n",
    "        # mask 0 with -infinity so it becomes 0 after softmax, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score_m = score_n.masked_fill(mask == 0, -1e10)    \n",
    "        \n",
    "        # softmax scores along each query, output dim: (batch_size x number_heads x seq_len x seq_len)\n",
    "        score_w = nn.functional.softmax(score_m, dim=-1) \n",
    "        \n",
    "        # multiply with value matrix: output weighted sum for each query, output dim: (batch_size x number_heads x seq_len x att_head_dim)\n",
    "        weighted_sum = torch.matmul(score_w, value)\n",
    "        \n",
    "        # concatenate attention heads to 1 output, output dim: (batch_size x number_heads x model_dimension)\n",
    "        weighted_sum = weighted_sum.permute(0, 2, 1, 3).contiguous().view(weighted_sum.shape[0], -1, self.number_heads * self.att_head_dim)\n",
    "        \n",
    "        # linear embedding for output\n",
    "        out = self.lin_output(weighted_sum)      \n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c977081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, model_dimension, hidden_dimension):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        \n",
    "        # linear layer\n",
    "        self.linear1 = nn.Linear(model_dimension, hidden_dimension)\n",
    "        self.linear2 = nn.Linear(hidden_dimension, model_dimension)\n",
    "        # non-linearity\n",
    "        self.non_linear = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.non_linear(self.linear1(x)))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "870b484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder stacks together all the previous modules\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_dimension=EMBED_SIZE, number_heads=12, ff_hidden_dim=EMBED_SIZE*4):\n",
    "        super(Encoder, self).__init__()\n",
    "        # attention heads\n",
    "        self.multihead_attention = MultiHeadAttention (number_heads, model_dimension)\n",
    "        # normalisation layer\n",
    "        self.normlayer = nn.LayerNorm(model_dimension)\n",
    "        self.feedforward_layer = FeedForwardLayer(model_dimension, hidden_dimension=ff_hidden_dim)\n",
    "    \n",
    "    # also residuals possible here\n",
    "    def forward(self, x, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        # input x 3x to generate query, key, value\n",
    "        x = self.normlayer(self.multihead_attention(x, x, x, mask))\n",
    "        return self.normlayer(self.feedforward_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "82f4474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class according to task sheet\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dimension, number_layers=12, number_heads=12):\n",
    "        super().__init__()\n",
    "        self.model_dimension=model_dimension\n",
    "        self.number_layers=number_layers\n",
    "        self.number_heads=number_heads\n",
    "        # hidden layer dimenion of FF is 4*model_dimension (see paper)\n",
    "        self.ff_hidden_layer = 4*model_dimension\n",
    "        # embedding of input \n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, seq_len=SEQ_LEN, embed_size=model_dimension)\n",
    "        # stack encoders\n",
    "        self.encoders = torch.nn.ModuleList() # create empty module list\n",
    "        for _ in range(self.number_layers):\n",
    "            self.encoders.append(Encoder(model_dimension=model_dimension, number_heads=number_heads, ff_hidden_dim=4*model_dimension))\n",
    "        \n",
    "    def forward(self, x, segment_info):\n",
    "        # mask to mark the padded tokens\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1,x.size(1),1).unsqueeze(1)\n",
    "        x = self.embedding(x, segment_info) # copied: what is segment_info? to be changed\n",
    "        # run trough encoders\n",
    "        for encoder in self.encoders:\n",
    "            x =encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "43b81c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining\n",
    "class MaskedPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    This class predicts the original token which was replaced by a mask. \n",
    "    \"\"\"\n",
    "    def __init__(self, bert_out, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(bert_out, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "2732c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    class to predict two classes: is next, not_next\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_out):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(bert_out, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "33c67f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning\n",
    "class ToxicityPrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    class to predict multivariate class of toxicity\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_out):\n",
    "        super().__init__()\n",
    "        self.tox_classes = 6 # there are 6 classes of toxicity in the dataset\n",
    "        self.linear = nn.Linear(bert_out, self.tox_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "66999e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuse to one model \n",
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    pertraining: masked token prediction, next sentence prediction, \n",
    "    finetuning: toxic comment prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, vocab_size):\n",
    "        super().__init__()\n",
    "        # base BERT model\n",
    "        self.base_model = base_model\n",
    "        # masked token classfication layer\n",
    "        self.masked_pred = MaskedPrediction(self.base_model.model_dimension, vocab_size)\n",
    "        # next sentence predicton layer\n",
    "        self.next_sentence = NextSentencePrediction(self.base_model.model_dimension)\n",
    "        # toxic comment classfication layer\n",
    "        self.toxic_comment = ToxicityPrediction(self.base_model.model_dimension)\n",
    "    \n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.base_model(x, segment_label)\n",
    "        return self.next_sentence(x), self.masked_pred(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0c699",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "afb04877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# copied: reproduce\n",
    "class TrainBERT:\n",
    "    def __init__(self, model, train_dataloader, test_dataloader=None, learning_rate=1e-4, weight_decay=0.01, betas=(0.9, 0.999), log_freq=10, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.training_data = train_dataloader\n",
    "        self.testing_data = test_dataloader\n",
    "\n",
    "        # optimizer: Adam\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # cost function negative log likelihood loss for masked token prediction\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def training(self, epoch):\n",
    "        self.iteration(epoch, self.training_data)\n",
    "\n",
    "    def testing(self, epoch):\n",
    "        self.iteration(epoch, self.testing_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        avg_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_element = 0\n",
    "\n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            next_sent_output, mask_lm_output = self.model.forward(data[\"input\"], data[\"segment\"])\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"label\"])\n",
    "\n",
    "            # 2-3. Adding next_loss and mask_loss: 3.4 Pre-training Procedure\n",
    "            loss = next_loss + mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            correct = next_sent_output.argmax(dim=-1).eq(data[\"is_next\"]).sum().item()\n",
    "            avg_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_element += data[\"is_next\"].nelement()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"avg_acc\": total_correct / total_element * 100,\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix)\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"EP{epoch}, {mode}: \\\n",
    "            avg_loss={avg_loss / len(data_iter)}, \\\n",
    "            total_acc={total_correct * 100.0 / total_element}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "e2a6901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (C:/Users/Johannes/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 131956802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EP_train:0:   0%|| 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2002, 6973, 1998, 2059, 11361, 28339, 2014, 2388, 1005, 1055, 2608, 2005, 2014, 2269, 1005, 1055, 103, 1010, 2029, 103, 12756, 103, 103]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2612, 0, 0, 2081, 0, 2868, 1012]\n",
      "[2002, 2001, 2107, 1037, 2158, 1005, 1055, 2158, 2525, 103]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1012]\n",
      "[2002, 3866, 2068, 103, 2172, 2000, 2022, 6380, 103]\n",
      "[0, 0, 0, 2205, 0, 0, 0, 0, 1012]\n",
      "[1036, 1036, 2182, 2057, 2024, 1010, 103, 103, 2016, 2056, 27726, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 1005, 1005, 0, 0, 0, 0]\n",
      "[2012, 2008, 2051, 1010, 2016, 2018, 2014, 2540, 2275, 2006, 2183, 2000, 103, 2082, 1998, 3352, 103, 3460, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2966, 0, 0, 0, 1037, 0, 0]\n",
      "[2013, 1996, 2051, 2016, 103, 1037, 2210, 103, 1010, 2016, 2018, 2359, 2498, 103, 2084, 2000, 2393, 2111, 1012]\n",
      "[0, 0, 0, 0, 2001, 0, 0, 2611, 0, 0, 0, 0, 0, 2062, 0, 0, 0, 0, 0]\n",
      "[2096, 2009, 2018, 2042, 2053, 3160, 2008, 2016, 2359, 2032, 2004, 23834, 2005, 6701, 1010, 2016, 2018, 2042, 5186, 8686, 2043, 2002, 1998, 2010, 2564, 1010, 5616, 1010, 2018, 2356, 2014, 2000, 2022, 103, 103, 1010, 7240, 1005, 1055, 1010, 2643, 18938, 5886, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5616, 0, 0, 0, 0, 0, 0, 2037, 2365, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2016, 3866, 2014, 14751, 5542, 2200, 2172, 1998, 103, 2000, 103, 1996, 103, 2643, 18938, 5886, 2016, 2071, 2005, 2032, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 3740, 0, 2022, 0, 2190, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2016, 14033, 2098, 103, 4615, 2005, 2652, 1036, 1036, 2902, 13472, 1005, 1005]\n",
      "[0, 0, 0, 2652, 0, 0, 0, 0, 0, 0, 1012, 0, 0]\n",
      "[2043, 2016, 2001, 2007, 2032, 1010, 103, 2018, 2210, 2051, 103, 5702, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 2016, 0, 0, 0, 2005, 0, 0]\n",
      "[5977, 8969, 2014, 2007, 1037, 2048, 4344, 103, 2077, 27987, 2075, 2039, 1998, 4815, 2091, 1996, 11202, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 17664, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[12756, 2499, 2000, 22512, 6701, 2046, 1996, 2482, 2835, 1999, 2014, 103, 1005, 2455, 13631, 103]\n",
      "[0, 0, 0, 2131, 0, 0, 0, 0, 0, 0, 0, 3008, 0, 0, 0, 1012]\n",
      "[3110, 103, 2016, 2001, 2320, 2153, 103, 10563, 1010, 2016, 6406, 103, 2014, 3008, 103, 2027, 3753, 2046, 1996, 7381, 103]\n",
      "[0, 2066, 0, 0, 0, 0, 1037, 0, 0, 0, 0, 2369, 0, 0, 2004, 0, 0, 0, 0, 0, 1012]\n",
      "[103, 2008, 1010, 103, 21695, 1871, 25628, 21041, 103, 4699, 1999, 2151, 1997, 1996, 4620, 1998, 22028, 103, 2741, 1012]\n",
      "[2044, 0, 0, 2002, 2018, 1050, 1005, 1056, 2042, 0, 0, 0, 0, 0, 0, 0, 0, 12756, 2741, 0]\n",
      "[2028, 2154, 2043, 2027, 2018, 2037, 2219, 2173, 2153, 1010, 2016, 2052, 2131, 2032, 1037, 3899, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[28448, 2001, 1037, 8367, 2008, 2018, 5881, 2007, 2032, 4316, 2085, 2008, 103, 2001, 4228, 1011, 2176, 103, 2496, 1012]\n",
      "[2009, 0, 0, 0, 0, 0, 0, 0, 0, 2130, 0, 0, 2002, 0, 0, 0, 0, 1998, 0, 0]\n",
      "[2016, 2404, 2006, 2014, 21944, 1011, 1037, 2152, 103, 7665, 103, 2013, 2014, 4470, 12643, 1010, 103, 1036, 1036, 10792, 1005, 1005, 1010, 2004, 2016, 103, 2170, 2032, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 2082, 0, 5592, 0, 0, 0, 0, 0, 2030, 0, 0, 0, 0, 0, 0, 0, 0, 2411, 0, 0, 0]\n",
      "[12643, 2001, 2014, 2388, 1005, 1055, 3336, 2567, 1998, 2069, 2365, 1997, 1996, 2155, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[6729, 2830, 103, 2014, 2835, 1010, 12756, 7168, 1996, 5119, 2006, 1996, 24923, 1012]\n",
      "[0, 0, 1999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[103, 103, 1050, 1005, 1056, 4527, 2000, 2156, 25085, 103, 3369, 2431, 2019, 103, 2077, 103, 18336, 2318, 1012]\n",
      "[2016, 2001, 0, 0, 0, 0, 0, 0, 2027, 2018, 0, 0, 0, 3178, 0, 1996, 0, 0, 0]\n",
      "[2014, 3008, 19252, 2247, 2000, 103, 2060, 2004, 2027, 19537, 2037, 2126, 2083, 1996, 3392, 1011, 7732, 9435, 2073, 103, 2018, 4961, 2039, 1012]\n",
      "[0, 0, 0, 0, 0, 2169, 0, 0, 0, 2081, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12756, 0, 0, 0, 1012]\n",
      "[2010, 8899, 8855, 2606, 1998, 2630, 2159, 2020, 3294, 5106, 1012]\n",
      "[0, 0, 8855, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2013, 1996, 2051, 2016, 2001, 1037, 2210, 2611, 1010, 2016, 2018, 2359, 2498, 103, 2084, 2000, 2393, 2111, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2062, 0, 0, 0, 0, 0]\n",
      "[2043, 2027, 2081, 2009, 2000, 1996, 3829, 1010, 12756, 5864, 2000, 4608, 2014, 3052, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1036, 1036, 3201, 103, 1005, 1005]\n",
      "[0, 0, 0, 1029, 0, 0]\n",
      "[2043, 2016, 2001, 2141, 1010, 103, 2001, 103, 2809, 1998, 1037, 103, 1012]\n",
      "[0, 0, 0, 0, 0, 2002, 0, 2069, 0, 0, 0, 2431, 0]\n",
      "[2096, 2070, 103, 2298, 2006, 2014, 2004, 2383, 1037, 2928, 2114, 2014, 2839, 2108, 2019, 4895, 15557, 2388, 1010, 103, 2018, 2973, 1037, 4659, 2512, 1011, 22614, 2166, 1012]\n",
      "[0, 0, 2453, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2016, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1036, 1036, 3201, 1029, 1005, 1005]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[13718, 1010, 2016, 2071, 1050, 1005, 1056, 2360, 2008, 2014, 2034, 2293, 2001, 4482, 1010, 6701, 1005, 1055, 2269, 103]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1012]\n",
      "[2612, 1010, 2009, 2001, 103, 2374, 2447, 1010, 2023, 2051, 103, 2770, 470, 2012, 1057, 3654, 103, 2040, 4110, 1998, 2101, 3631, 103, 2540, 1037, 2095, 2101, 1012]\n",
      "[0, 0, 0, 0, 2178, 0, 0, 0, 0, 0, 1037, 0, 2067, 0, 0, 0, 1010, 0, 0, 0, 0, 0, 2014, 0, 0, 0, 0, 0]\n",
      "[2016, 103, 1050, 1005, 1056, 4527, 2000, 2156, 2027, 2018, 3369, 2431, 2019, 3178, 2077, 1996, 18336, 2318, 1012]\n",
      "[0, 2001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2028, 2518, 2014, 2388, 103, 103, 2841, 2006, 2001, 2108, 2006, 2051, 103, 18435, 103, 2192, 1012]\n",
      "[0, 0, 0, 0, 6620, 2094, 0, 0, 0, 0, 0, 0, 1998, 0, 1037, 0, 0]\n",
      "[6701, 2001, 103, 25719, 2125, 103, 6093, 1999, 103, 103, 3635, 2429, 2000, 2010, 23614, 2937, 1012]\n",
      "[0, 0, 2525, 0, 0, 1996, 0, 0, 4578, 1998, 0, 2429, 0, 0, 0, 0, 0]\n",
      "[4482, 103, 2464, 6701, 2069, 3807, 1999, 2010, 6480, 1011, 1996, 2154, 2002, 2018, 2042, 2141, 1998, 1996, 2154, 2002, 2234, 2188, 2013, 1996, 103, 1012]\n",
      "[0, 2018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2154, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2902, 0]\n",
      "[12756, 3368, 1012]\n",
      "[0, 0, 0]\n",
      "[2002, 2001, 2107, 1037, 2158, 1005, 1055, 103, 2525, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2158, 0, 0]\n",
      "[2008, 103, 103, 12756, 1005, 1055, 2933, 2043, 2016, 103, 2032, 5102, 3041, 103]\n",
      "[0, 2018, 2042, 0, 0, 0, 0, 0, 0, 2288, 0, 0, 0, 1012]\n",
      "[103, 1005, 1040, 2464, 1996, 3185, 2471, 2011, 6707, 1010, 6195, 2002, 2001, 1037, 103, 2402, 2005, 1996, 18720, 9476, 1010, 2021, 103, 3080, 12334, 1010, 2247, 2007, 2014, 3428, 1010, 6701, 2001, 2411, 6086, 2000, 2477, 2008, 2020, 3080, 1012]\n",
      "[2002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2210, 0, 0, 0, 0, 0, 0, 0, 2007, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[103, 2001, 1050, 1005, 1056, 2127, 2016, 3062, 27583, 2293, 2005, 1996, 2034, 2051, 1999, 2014, 2166, 2008, 2016, 4711, 2673, 2185, 1012]\n",
      "[2009, 0, 0, 0, 0, 0, 0, 0, 1999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2014, 3008, 19252, 2247, 2000, 2169, 2060, 2004, 2027, 2081, 2037, 2126, 2083, 1996, 3392, 1011, 7732, 9435, 2073, 12756, 2018, 4961, 2039, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2126, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2612, 103, 2016, 2787, 2008, 2016, 2052, 2468, 1037, 6821, 103, 2029, 2052, 13883, 2014, 103, 103, 2729, 2005, 5305, 2111, 103]\n",
      "[2612, 1010, 0, 0, 0, 0, 0, 0, 0, 0, 1010, 0, 0, 0, 0, 2342, 2000, 0, 0, 0, 0, 1012]\n",
      "[1997, 2607, 1010, 2014, 3276, 2007, 4482, 3092, 2039, 4315, 29544, 103, 103, 7665, 2043, 2016, 2288, 6875, 14153, 1012]\n",
      "[0, 0, 0, 2014, 0, 0, 0, 0, 0, 0, 0, 3859, 2077, 0, 0, 0, 0, 6875, 0, 0]\n",
      "[1036, 1036, 1045, 2215, 103, 3298, 11861, 1005, 1005, 103, 2056, 1012]\n",
      "[0, 0, 0, 0, 2000, 0, 1010, 0, 0, 9448, 0, 0]\n",
      "[2007, 1037, 15081, 1010, 5977, 3880, 1010, 103, 103, 15725, 1045, 1005, 1049, 2175, 2078, 6583, 2292, 2017, 3298, 2026, 2482, 1012, 1005, 1005]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1036, 1036, 2066, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2788, 1010, 2002, 2052, 2022, 13311, 103, 1996, 103, 2282, 1010, 2652, 2007, 2010, 10899, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 2105, 0, 2542, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2144, 1996, 3185, 2001, 2471, 2058, 1010, 103, 2354, 2016, 2488, 103, 2046, 1996, 103, 103, 3926, 2893, 3201, 103]\n",
      "[0, 0, 0, 0, 0, 0, 0, 12756, 0, 0, 0, 7540, 0, 0, 5010, 1998, 0, 0, 0, 1012]\n",
      "[2004, 2027, 103, 2046, 1996, 2277, 1010, 2014, 2388, 2584, 2005, 103, 1012]\n",
      "[0, 0, 2318, 0, 0, 0, 0, 0, 0, 0, 0, 6701, 0]\n",
      "[1036, 1036, 2057, 1005, 2222, 103, 2032, 103, 2017, 2064, 103, 2156, 2065, 5616, 3791, 2151, 2393, 1012, 1005, 1005]\n",
      "[0, 0, 0, 0, 0, 2202, 0, 2061, 0, 0, 2175, 0, 0, 0, 0, 0, 2393, 0, 0, 0]\n",
      "[2002, 2356, 1012]\n",
      "[0, 0, 0]\n",
      "[2002, 2356, 1012]\n",
      "[0, 0, 0]\n",
      "[2043, 103, 2234, 2051, 2005, 2014, 2000, 2831, 1010, 2016, 2074, 2071, 1050, 1005, 1056, 4025, 103, 103, 1036, 1036, 4470, 12643, 1005, 1005, 2041, 1012]\n",
      "[0, 2009, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2000, 2131, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2612, 1010, 2016, 2170, 2032, 1036, 1036, 10792, 1012, 1005, 1005]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2028, 2518, 2014, 2388, 6620, 2094, 2841, 2006, 2001, 103, 2006, 2051, 1998, 103, 1037, 2192, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 2108, 0, 0, 0, 18435, 0, 0, 0]\n",
      "[2612, 1010, 2016, 103, 2008, 2016, 2052, 2468, 1037, 6821, 1010, 2029, 2052, 13883, 2014, 2342, 2000, 2729, 2005, 5305, 103, 1012]\n",
      "[0, 0, 0, 2787, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2111, 0]\n",
      "[2043, 2016, 2001, 2007, 2032, 1010, 2016, 2018, 2210, 103, 2005, 103, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 2051, 0, 5702, 0]\n",
      "[2007, 2014, 7022, 2525, 1999, 1996, 11848, 103, 2016, 2001, 4895, 28139, 19362, 2098, 2005, 1996, 6832, 12554, 2016, 5281, 2043, 3765, 6038, 3631, 2039, 2007, 2014, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1010, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2043, 103, 2001, 2141, 1010, 2002, 103, 2069, 2809, 1998, 1037, 2431, 1012]\n",
      "[0, 2016, 0, 0, 0, 0, 2001, 0, 0, 0, 0, 0, 0]\n",
      "[2612, 1010, 2009, 103, 2178, 2374, 2447, 1010, 2023, 2051, 1037, 2770, 2067, 2012, 1057, 3654, 1010, 2040, 4110, 1998, 2101, 3631, 2014, 2540, 1037, 2095, 2101, 1012]\n",
      "[0, 0, 0, 2001, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1036, 1036, 2748, 1010, 2017, 1005, 2128, 2893, 2000, 2022, 2107, 1037, 2502, 1010, 103, 103, 1012, 1005, 1005]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1037, 0, 0, 3082, 2879, 0, 0, 0]\n",
      "[13879, 1010, 2016, 103, 103, 2000, 2465, 1998, 3092, 2039, 19857, 8950, 2075, 103, 13609, 1012]\n",
      "[0, 0, 0, 3030, 2183, 0, 0, 0, 0, 0, 0, 0, 0, 1996, 0, 0]\n",
      "[3061, 2067, 2013, 1996, 5259, 1010, 2016, 2357, 2000, 1998, 10424, 2080, 2000, 2202, 1999, 2014, 3311, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[103, 2001, 1037, 2261, 2086, 103, 2013, 2043, 2016, 2018, 2761, 3740, 2006, 6800, 1010, 2021, 2016, 2001, 7568, 2044, 2673, 2018, 2008, 2018, 3047, 1010, 103, 2001, 103, 103, 1012]\n",
      "[2016, 0, 0, 0, 0, 2125, 0, 0, 2016, 0, 0, 0, 2006, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2016, 0, 2633, 5131, 0]\n",
      "[12643, 2001, 2014, 103, 1005, 1055, 3336, 2567, 1998, 2069, 103, 1997, 103, 2155, 1012]\n",
      "[0, 0, 0, 2388, 0, 0, 0, 0, 0, 0, 2365, 0, 1996, 2155, 0]\n",
      "[2043, 2016, 2001, 2141, 1010, 2002, 2001, 2069, 2809, 1998, 1037, 2431, 1012]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/4 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[460], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 19\u001b[0m   bert_trainer\u001b[38;5;241m.\u001b[39mtraining(epoch)\n",
      "Cell \u001b[1;32mIn[453], line 24\u001b[0m, in \u001b[0;36mTrainBERT.training\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration(epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_data)\n",
      "Cell \u001b[1;32mIn[453], line 67\u001b[0m, in \u001b[0;36mTrainBERT.iteration\u001b[1;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     66\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# next sentence prediction accuracy\u001b[39;00m\n\u001b[0;32m     70\u001b[0m correct \u001b[38;5;241m=\u001b[39m next_sent_output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_next\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''test run'''\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_data, _ = load_data(\"bookcorpus\", transformation=tokenizer, n_train=100)\n",
    "# train_data = Bookcorpus(tokenizer, n_rows = 100)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "   train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "# def __init__(self, vocab_size=VOCAB_SIZE, model_dimension=EMBED_SIZE, number_layers=12, number_heads=12):\n",
    "bert_model = Model(\n",
    "  vocab_size=VOCAB_SIZE,\n",
    "  model_dimension=EMBED_SIZE\n",
    ")\n",
    "\n",
    "bert_lm = BERT(bert_model, len(tokenizer.vocab))\n",
    "bert_trainer = TrainBERT(bert_lm, train_loader, device='cpu')\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.training(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada6b40",
   "metadata": {},
   "source": [
    "## Functions for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class BertTokenizer():\n",
    "    def __init__(self, task_type=\"pretrain\"):\n",
    "        if not task_type in [\"pretrain\", \"text_classification_multi\"]:\n",
    "            raise ValueError(\"task not implemented\")\n",
    "        pass\n",
    "    \n",
    "    def __call__()\"\"\"\n",
    "# i noticed we dont need any callable class to do transformation on the datasets since everything is handeled by our dataloaders\n",
    "# ie we dont need rescaling etc.\n",
    "# maybe ask supervisor if we need to save back the tokenized text or if it is okay to do it on the fly and leave the load_data transformation parameter at None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "7bf4a2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def __init__(self, tokenizer, seq_len=64, split=\"train\", n_rows=None):\n",
    "\n",
    "def load_data(dataset:str, transformation=None, n_train:int=None, n_test:int=None): # transformation callable\n",
    "    \n",
    "    if dataset == \"bookcorpus\":\n",
    "        train = Bookcorpus(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        return train, None\n",
    "    \n",
    "    elif dataset == \"jigsaw_toxicity_pred\":\n",
    "        train = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"train\",\n",
    "            n_rows=n_train\n",
    "        )\n",
    "        \n",
    "        test = ToxicComment(\n",
    "            tokenizer=transformation,\n",
    "            seq_len=SEQ_LEN,\n",
    "            split=\"test\",\n",
    "            n_rows=n_test\n",
    "        )\n",
    "        return train, test\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"jigsaw_toxicity_pred\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fb18c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(iter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac722bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train, test = load_data(\"bookcorpus\", transformation=tokenizer, n_train=1000, n_test=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762923b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1552ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x, outfile:str=None): # can have more args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
