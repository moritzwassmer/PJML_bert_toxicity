{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97783",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* think about visualizations for text, preprocessing text, etc.\n",
    "* cleaner code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802591b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5848b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64 # maximum sequence length\n",
    "VOCAB_SIZE = 30522  # = len(tokenizer.vocab)\n",
    "N_SEGMENTS = 3 # number of segmentation labels\n",
    "EMBED_SIZE = 768 # size of embedding vector\n",
    "DROPOUT = 0.1 # dropout chance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebacda",
   "metadata": {},
   "source": [
    "#### Tokenizer - use pretrained, at least for prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766ae484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model_max_length = SEQ_LEN # might not be correct in case of pretraining where we add CLS at the end, check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/preprocessing\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose an appropriate tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827bc7f",
   "metadata": {},
   "source": [
    "#### Tokenizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc5fe66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.truncation_side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5adf4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length # we might need to fixate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2db1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09c4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a719881",
   "metadata": {},
   "source": [
    "#### Tokenizer example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6afd8af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am moritz, who are you ?\"#[\"hi i am moritz\", \"no you are not moritz, you are kevin\"]\n",
    "encoded_input = tokenizer(text)#,padding=True, truncation=True)\n",
    "# , return_tensors='pt') use this for pt tensors\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432fff51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7632, 1045, 2572, 28461, 1010, 2040, 2024, 2017, 1029, 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f874a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hi i am moritz, who are you? [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e006",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Cant be downloaded automatically from huggingface. Needs to be downloaded manually:\n",
    "\n",
    "1) download from kaggle and \n",
    "2) extract in finetuning folder \n",
    "3) Delete the zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f30c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset jigsaw_toxicity_pred (C:/Users/Johannes/.cache/huggingface/datasets/jigsaw_toxicity_pred/default-ebae0308d0d3f840/1.1.0/9cf096ac4341c35839bc8a9f6a19d93e18e5ad3d84cf05f690d2bc6f7384af85)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed182c7da6f4b25aa483556ed70b36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 159571\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 63978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toxic_path = r\"C:\\Users\\morit\\OneDrive\\UNI\\Master\\WS23\\PML\\repo\\bert_from_scratch.toxic_comment\\datasets\\finetuning\\kaggle-toxic_comment\"\n",
    "toxic_path = r\"C:\\Users\\Johannes\\Project Machine Learning\\datasets\\finetuning\\toxic_comment\"\n",
    "toxic_dataset = load_dataset(\"jigsaw_toxicity_pred\", data_dir=toxic_path)\n",
    "toxic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd0356",
   "metadata": {},
   "source": [
    "#### Test with standard dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f394a9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 159571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comment_text': ['Reply to Berean Hunter. \\n\\nHi I found the source here. http://en.wikipedia.org/wiki/Battle_of_Trenton\\n\\nAnd for the battle of New Orleans I saw the source on the History Channel Documentary: First Invasion War of 1812.'],\n",
       " 'toxic': tensor([0]),\n",
       " 'severe_toxic': tensor([0]),\n",
       " 'obscene': tensor([0]),\n",
       " 'threat': tensor([0]),\n",
       " 'insult': tensor([0]),\n",
       " 'identity_hate': tensor([0])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "dataset_length = len(toxic_dataset[\"train\"])\n",
    "print(\"Length of dataset:\", dataset_length)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "068cf9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(toxic_dataset[\"train\"], batch_size=1, shuffle = True)\n",
    "\n",
    "sums = {\n",
    " 'nothing' : 0,\n",
    " 'toxic': 0,\n",
    " 'severe_toxic': 0,\n",
    " 'obscene': 0,\n",
    " 'threat': 0,\n",
    " 'insult': 0,\n",
    " 'identity_hate': 0\n",
    "}\n",
    "\n",
    "for i in range(dataset_length):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    batch = next(iter(dataloader))\n",
    "    for key in batch.keys():\n",
    "        if key in sums:\n",
    "            sums[key] += batch[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "432c0612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 0,\n",
       " 'toxic': tensor([87]),\n",
       " 'severe_toxic': tensor([7]),\n",
       " 'obscene': tensor([45]),\n",
       " 'threat': tensor([3]),\n",
       " 'insult': tensor([36]),\n",
       " 'identity_hate': tensor([4])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af29e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 0.013027581601785167,\n",
       " 'toxic': 0.1221035512154764,\n",
       " 'severe_toxic': 1.170816120557678,\n",
       " 'obscene': 0.22102635960344377,\n",
       " 'threat': 3.9068027453755154,\n",
       " 'insult': 0.23707651546140618,\n",
       " 'identity_hate': 1.3291471261846948}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_length = 159571\n",
    "weights = {\n",
    " 'nothing' : 1/(143346/dataset_length),\n",
    " 'toxic': 1/(15294/dataset_length),\n",
    " 'severe_toxic':  1/(1595/dataset_length),\n",
    " 'obscene':  1/(8449/dataset_length),\n",
    " 'threat':  1/(478/dataset_length),\n",
    " 'insult':  1/(7877/dataset_length),\n",
    " 'identity_hate':  1/(1405/dataset_length)\n",
    "}\n",
    "\n",
    "summe = sum(weights.values())\n",
    "\n",
    "for key in weights.keys():\n",
    "    weights[key] = weights[key]/(summe/7)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "768afc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 5.623175898321041,\n",
       " 'toxic': 0.5999529264082849,\n",
       " 'severe_toxic': 0.06256864898791778,\n",
       " 'obscene': 0.3314373136670328,\n",
       " 'threat': 0.018750980699827394,\n",
       " 'insult': 0.30899890161619326,\n",
       " 'identity_hate': 0.055115330299701865}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_length = 159571\n",
    "weights = {\n",
    " 'nothing' : (143346/dataset_length),\n",
    " 'toxic': (15294/dataset_length),\n",
    " 'severe_toxic':  (1595/dataset_length),\n",
    " 'obscene':  (8449/dataset_length),\n",
    " 'threat':  (478/dataset_length),\n",
    " 'insult':  (7877/dataset_length),\n",
    " 'identity_hate':  (1405/dataset_length)\n",
    "}\n",
    "\n",
    "summe = sum(weights.values())\n",
    "\n",
    "for key in weights.keys():\n",
    "    weights[key] = weights[key]/(summe/7)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84275e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(weights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e663bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 0.8983211235124177,\n",
       " 'toxic': 10.433568719759382,\n",
       " 'severe_toxic': 100.04451410658307,\n",
       " 'obscene': 18.886377086045687,\n",
       " 'threat': 333.8305439330544,\n",
       " 'insult': 20.25783927891329,\n",
       " 'identity_hate': 113.57366548042705}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_length = 159571\n",
    "weights = {\n",
    " 'nothing' : (143346/TRAIN_TOTAL),\n",
    " 'toxic': (TRAIN_TOTAL/15294),\n",
    " 'severe_toxic':  (TRAIN_TOTAL/1595),\n",
    " 'obscene':  (TRAIN_TOTAL/8449),\n",
    " 'threat':  (TRAIN_TOTAL/478),\n",
    " 'insult':  (TRAIN_TOTAL/7877),\n",
    " 'identity_hate':  (TRAIN_TOTAL/1405)\n",
    "}\n",
    "\n",
    "summe = sum(weights.values())\n",
    "\n",
    "for key in weights.keys():\n",
    "    weights[key] = weights[key]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddaa7d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': 1.4905098171084832,\n",
       " 'severe_toxic': 14.292073443797582,\n",
       " 'obscene': 2.698053869435098,\n",
       " 'threat': 47.69007770472206,\n",
       " 'insult': 2.893977039844756,\n",
       " 'identity_hate': 16.22480935434672}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight_for_class_i = total_samples / (num_samples_in_class_i * num_classes)\n",
    "NUM_CLASSES = 7\n",
    "TRAIN_TOTAL = 159571\n",
    "CLASS_WEIGHTS = {\n",
    " 'toxic': (TRAIN_TOTAL/(15294*NUM_CLASSES)),\n",
    " 'severe_toxic': (TRAIN_TOTAL/(1595*NUM_CLASSES)),\n",
    " 'obscene':  TRAIN_TOTAL/(8449*NUM_CLASSES),\n",
    " 'threat':  TRAIN_TOTAL/(478*NUM_CLASSES),\n",
    " 'insult': TRAIN_TOTAL/(7877*NUM_CLASSES),\n",
    " 'identity_hate':  TRAIN_TOTAL/(1405*NUM_CLASSES)\n",
    "}\n",
    "\n",
    "for key in CLASS_WEIGHTS.keys():\n",
    "    CLASS_WEIGHTS[key] = CLASS_WEIGHTS[key]\n",
    "CLASS_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb9de29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.2895012292547"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(CLASS_WEIGHTS.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
